title,publication_date,source,url,description,ai_keywords_found,ce_keywords_found
"Automation, controls leading concerns for hospital engineers and designers",2025-12-23,Civil + Structural Engineer Media,https://www.csemag.com/automation-controls-leading-concerns-for-hospital-engineers-and-designers/,"<p>Building automation and controls are used in hospitals and health care facilities, and automated systems are enhancing the patient experience.</p>
<p>The post <a href=""https://www.csemag.com/automation-controls-leading-concerns-for-hospital-engineers-and-designers/"">Automation, controls leading concerns for hospital engineers and designers</a> appeared first on <a href=""https://www.csemag.com"">Consulting - Specifying Engineer</a>.</p>","artificial intelligence, automation",infrastructure
How MEP/FP engineers are living a dual reality,2025-12-12,Civil + Structural Engineer Media,https://www.csemag.com/how-mep-fp-engineers-are-living-a-dual-reality/,"<p>This year’s Salary Report reveals high-paying jobs with lots of pitfalls for MEP/FP engineers. </p>
<p>The post <a href=""https://www.csemag.com/how-mep-fp-engineers-are-living-a-dual-reality/"">How MEP/FP engineers are living a dual reality</a> appeared first on <a href=""https://www.csemag.com"">Consulting - Specifying Engineer</a>.</p>",artificial intelligence,bridge
"What codes, standards keep hospital designers up at night?",2025-12-11,Civil + Structural Engineer Media,https://www.csemag.com/what-codes-standards-keep-hospital-designers-up-at-night/,"<p>Learn about the key hospital and health care facility codes and standards from ASHRAE, NFPA, FGI and other associations.</p>
<p>The post <a href=""https://www.csemag.com/what-codes-standards-keep-hospital-designers-up-at-night/"">What codes, standards keep hospital designers up at night?</a> appeared first on <a href=""https://www.csemag.com"">Consulting - Specifying Engineer</a>.</p>",automation,"construction, infrastructure"
Design health care facilities with energy efficiency and flexibility in mind,2025-12-09,Civil + Structural Engineer Media,https://www.csemag.com/design-health-care-facilities-with-energy-efficiency-and-flexibility-in-mind/,"<p>In this roundtable, engineers discuss current trends for health care facilities and where the industry is going in the coming years.  </p>
<p>The post <a href=""https://www.csemag.com/design-health-care-facilities-with-energy-efficiency-and-flexibility-in-mind/"">Design health care facilities with energy efficiency and flexibility in mind</a> appeared first on <a href=""https://www.csemag.com"">Consulting - Specifying Engineer</a>.</p>","robotics, automation","construction, structural, infrastructure, concrete"
"AI and digital tools can empower civil engineers, not replace them",2025-12-23,New Civil Engineer,https://www.newcivilengineer.com/opinion/ai-and-digital-tools-can-empower-civil-engineers-not-replace-them-23-12-2025/,"<p>Artificial intelligence (AI) is already reshaping civil engineering practice, but the sector must approach these tools with confidence rather than fear.</p>
<p>The post <a href=""https://www.newcivilengineer.com/opinion/ai-and-digital-tools-can-empower-civil-engineers-not-replace-them-23-12-2025/"">AI and digital tools can empower civil engineers, not replace them</a> appeared first on <a href=""https://www.newcivilengineer.com"">New Civil Engineer</a>.</p>","artificial intelligence, machine learning","construction, infrastructure, concrete"
Early careers rising stars discuss balancing productivity push with climate and environmental challenges,2025-12-23,New Civil Engineer,https://www.newcivilengineer.com/in-depth/early-careers-rising-stars-discuss-balancing-productivity-push-with-climate-and-environmental-challenges-23-12-2025/,"<p>Faced with competing needs to drive productivity and growth while managing decarbonisation and climate change, the sector is always seeking new ideas – and this year’s crop of Graduates and Apprentices of the Year are undaunted by the task.</p>
<p>The post <a href=""https://www.newcivilengineer.com/in-depth/early-careers-rising-stars-discuss-balancing-productivity-push-with-climate-and-environmental-challenges-23-12-2025/"">Early careers rising stars discuss balancing productivity push with climate and environmental challenges</a> appeared first on <a href=""https://www.newcivilengineer.com"">New Civil Engineer</a>.</p>",artificial intelligence,"construction, infrastructure, bridge"
Heathrow to begin £1.3bn Terminal upgrades and new baggage system work in 2026,2025-12-22,New Civil Engineer,https://www.newcivilengineer.com/latest/heathrow-to-begin-1-3bn-terminal-upgrades-and-new-baggage-system-work-in-2026-22-12-2025/,"<p>Heathrow Airport has announced £1.3bn of infrastructure projects due to start in 2026 that it says will improve reliability and accessibility for passengers.</p>
<p>The post <a href=""https://www.newcivilengineer.com/latest/heathrow-to-begin-1-3bn-terminal-upgrades-and-new-baggage-system-work-in-2026-22-12-2025/"">Heathrow to begin £1.3bn Terminal upgrades and new baggage system work in 2026</a> appeared first on <a href=""https://www.newcivilengineer.com"">New Civil Engineer</a>.</p>",artificial intelligence,"construction, infrastructure"
Trimble introduces SketchUp AI,2025-12-11,AEC Magazine,https://aecmag.com/cad/trimble-introduces-sketchup-ai/,"<p>Subscription-based service includes generative AI image creation and text to 3D capabilities</p>
<p>The post <a href=""https://aecmag.com/cad/trimble-introduces-sketchup-ai/"" rel=""nofollow"">Trimble introduces SketchUp AI</a> appeared first on <a href=""https://aecmag.com"" rel=""nofollow"">AEC Magazine</a>.</p>",generative AI,construction
A peek into Rayon’s AI future,2025-12-02,AEC Magazine,https://aecmag.com/cad/a-peek-into-rayons-ai-future/,"<p>We preview some of the promising new new AI tools for the “Figma of 2D CAD”</p>
<p>The post <a href=""https://aecmag.com/cad/a-peek-into-rayons-ai-future/"" rel=""nofollow"">A peek into Rayon’s AI future</a> appeared first on <a href=""https://aecmag.com"" rel=""nofollow"">AEC Magazine</a>.</p>","artificial intelligence, automation",construction
Views from the AEC data lake,2025-12-02,AEC Magazine,https://aecmag.com/data-management/views-from-the-aec-data-lake/,"<p>When transitioning away from proprietary files, some firms are seeking out cloud databases that promise genuine ownership of data</p>
<p>The post <a href=""https://aecmag.com/data-management/views-from-the-aec-data-lake/"" rel=""nofollow"">Views from the AEC data lake</a> appeared first on <a href=""https://aecmag.com"" rel=""nofollow"">AEC Magazine</a>.</p>","artificial intelligence, machine learning, automation","construction, infrastructure"
Trimble builds AI strategy around agentic AI platform,2025-11-27,AEC Magazine,https://aecmag.com/ai/trimble-builds-ai-strategy-around-agentic-ai-platform/,"<p>Agentic AI to be embedded across Trimble’s technology stack and within the workflows of customers and partners </p>
<p>The post <a href=""https://aecmag.com/ai/trimble-builds-ai-strategy-around-agentic-ai-platform/"" rel=""nofollow"">Trimble builds AI strategy around agentic AI platform</a> appeared first on <a href=""https://aecmag.com"" rel=""nofollow"">AEC Magazine</a>.</p>",automation,construction
Managing aging water infrastructure: a proactive approach,2025-11-18,AEC Magazine,https://aecmag.com/sponsored-content/managing-aging-water-infrastructure-a-proactive-approach/,"<p>By Keaton Clay, National Account Executive: Oldcastle Infrastructure CivilSense</p>
<p>The post <a href=""https://aecmag.com/sponsored-content/managing-aging-water-infrastructure-a-proactive-approach/"" rel=""nofollow"">Managing aging water infrastructure: a proactive approach</a> appeared first on <a href=""https://aecmag.com"" rel=""nofollow"">AEC Magazine</a>.</p>",artificial intelligence,"transportation, infrastructure, concrete"
Vibe Coding: The Good,2025-11-18,The BIMsider,https://thebimsider.com/2025/11/18/vibe-coding-good/,"The rush of getting code to vibe Remember your first hackathon? That wild, caffeine-fueled moment when code actually worked on the first try? Even if you’ve never been to one, you know the feeling when everything just clicks and ideas start to flow faster than you can type. Vibe coding feels like that, only this&#8230; <a class=""more-link"" href=""https://thebimsider.com/2025/11/18/vibe-coding-good/"">Continue reading <span class=""screen-reader-text"">Vibe Coding: The&#160;Good</span></a>",automation,construction
"Apple’s App Course Runs $20,000 a Student. Is It Really Worth It?",2025-12-24,Wired,https://www.wired.com/story/apple-app-making-course-michigan-state-university/,"Apple, Michigan taxpayers, and one of Detroit’s wealthiest families spent roughly $30 million training hundreds of people to build iPhone apps. Not everyone lands coding jobs right away.",generative AI,construction
Pinterest Users Are Tired of All the AI Slop,2025-12-24,Wired,https://www.wired.com/story/pinterst-ai-slop-content/,A surge of AI-generated content is frustrating Pinterest users and left some questioning whether the platform still works at all.,generative AI,bridge
"AlphaFold Changed Science. After 5 Years, It’s Still Evolving",2025-12-24,Wired,https://www.wired.com/story/alphafold-changed-science-after-5-years-its-still-evolving/,WIRED spoke with DeepMind’s Pushmeet Kohli about the recent past—and promising future—of the Nobel Prize-winning research project that changed biology and chemistry forever.,"artificial intelligence, neural networks","structural, concrete, bridge"
Big Balls Was Just the Beginning,2025-12-23,Wired,https://www.wired.com/story/expired-tired-wired-doge/,DOGE dominated the news this year as Elon Musk’s operatives shook up several US government agencies. It’s far from over.,artificial intelligence,infrastructure
"How Elon Musk Won His No Good, Very Bad Year",2025-12-22,Wired,https://www.wired.com/story/expired-tired-wired-tesla/,The billionaire’s involvement with the Trump administration and DOGE had deep impacts on Tesla’s bottom line. But Elon Musk was still able to turn his attention to SpaceX.,robotics,transportation
"While everyone talks about an AI bubble, Salesforce quietly added 6,000 enterprise customers in 3 months",2025-12-22,VentureBeat,https://venturebeat.com/technology/while-everyone-talks-about-an-ai-bubble-salesforce-quietly-added-6-000,"<p>While Silicon Valley debates whether artificial intelligence has become an <a href=""https://www.reuters.com/business/finance/opinions-split-over-ai-bubble-after-billions-invested-2025-10-16/"">overinflated bubble</a>, Salesforce&#x27;s enterprise AI platform quietly added 6,000 new customers in a single quarter — a 48% increase that executives say demonstrates a widening gap between speculative AI hype and deployed enterprise solutions generating measurable returns.</p><p><a href=""https://www.salesforce.com/agentforce/"">Agentforce</a>, the company&#x27;s autonomous AI agent platform, now serves 18,500 enterprise customers, up from 12,500 the prior quarter. Those customers collectively run more than three billion automated workflows monthly and have pushed Salesforce&#x27;s agentic product revenue past $540 million in annual recurring revenue, according to figures the company shared with VentureBeat. The platform has processed over three trillion tokens — the fundamental units that large language models use to understand and generate text — positioning Salesforce as one of the largest consumers of AI compute in the enterprise software market.</p><p>&quot;This has been a year of momentum,&quot; Madhav Thattai, Salesforce&#x27;s Chief Operating Officer for AI, said in an exclusive interview with VentureBeat. &quot;We crossed over half a billion in ARR for our agentic products, which have been out for a couple of years. And so that&#x27;s pretty remarkable for enterprise software.&quot;</p><p>The numbers arrive amid intensifying scrutiny of AI spending across corporate America. Venture capitalists and analysts have questioned whether the <a href=""https://www.wsj.com/tech/it-really-is-possible-to-spend-too-much-on-ai-7bb68df1"">billions pouring into AI infrastructure</a> — from data centers to graphics processing units to model development — will ever generate proportionate returns. <a href=""https://ai.meta.com/"">Meta</a>, <a href=""https://www.microsoft.com/en-us/"">Microsoft</a>, and <a href=""https://www.amazon.com/"">Amazon</a> have committed tens of billions to AI infrastructure, prompting some investors to ask whether the enthusiasm has outpaced the economics.</p><p>Yet the <a href=""https://www.salesforce.com/"">Salesforce</a> data suggests that at least one segment of the AI market — enterprise workflow automation — is translating investments into concrete business outcomes at a pace that defies the bubble narrative.</p><h2><b>Why enterprise AI trust has become the defining challenge for CIOs in 2025</b></h2><p>The distinction between AI experimentation and AI deployment at scale comes down to one word that appeared repeatedly across interviews with Salesforce executives, customers, and independent analysts: trust.</p><p><a href=""http://m"">Dion Hinchcliffe</a>, who leads the CIO practice at technology research firm <a href=""https://futurumgroup.com/"">The Futurum Group</a>, said the urgency around enterprise AI has reached a fever pitch not seen in previous technology cycles. His firm recently completed a <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">comprehensive analysis of agentic AI platforms</a> that ranked Salesforce slightly ahead of Microsoft as the market leader.</p><p>&quot;I&#x27;ve been through revolution after revolution in this business,&quot; Hinchcliffe said. &quot;I&#x27;ve never seen anything like this before. In my entire career, I&#x27;ve never seen this level of business focus—boards of directors are directly involved, saying this is existential for the company.&quot;</p><p>The pressure flows downward. CIOs who once managed technology as a cost center now field questions directly from board members demanding to know how their companies will avoid being disrupted by AI-native competitors.</p><p>&quot;They&#x27;re pushing the CIO hard, asking, &#x27;What are we doing? How do we make sure we&#x27;re not put out of business by the next AI-first company that reimagines what we do?&#x27;&quot; Hinchcliffe said.</p><p>But that pressure creates a paradox. Companies want to move fast on AI, yet the very autonomy that makes AI agents valuable also makes them dangerous. An agent that can independently execute workflows, process customer data, and make decisions without human intervention can also make mistakes at machine speed — or worse, be manipulated by bad actors.</p><p>This is where enterprise AI platforms differentiate themselves from the consumer AI tools that dominate headlines. According to Hinchcliffe, building a production-grade agentic AI system requires hundreds of specialized engineers working on governance, security, testing, and orchestration — infrastructure that most companies cannot afford to build themselves.</p><p>&quot;The average enterprise-grade agentic team is 200-plus people working on an agentic platform,&quot; Hinchcliffe said. &quot;Salesforce has over 450 people working on agent AI.&quot;</p><p>Early in the AI adoption cycle, many CIOs attempted to build their own agent platforms using open-source tools like <a href=""https://www.langchain.com/"">LangChain</a>. They quickly discovered the complexity exceeded their resources.</p><p>&quot;They very quickly realized this problem was much bigger than expected,&quot; Hinchcliffe explained. &quot;To deploy agents at scale, you need infrastructure to manage them, develop them, test them, put guardrails on them, and govern them — because you&#x27;re going to have tens of thousands, hundreds of thousands, even millions of long-running processes out there doing work.&quot;</p><h2><b>How AI guardrails and security layers separate enterprise platforms from consumer chatbots</b></h2><p>The technical architecture that separates enterprise AI platforms from consumer tools centers on what the industry calls a &quot;trust layer&quot; — a set of software systems that monitor, filter, and verify every action an AI agent attempts to take.</p><p>Hinchcliffe&#x27;s research found that <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">only about half of the agentic AI platforms</a> his firm evaluated included runtime trust verification — the practice of checking every transaction for policy compliance, data toxicity, and security violations as it happens, rather than relying solely on design-time constraints that can be circumvented.</p><p>&quot;Salesforce puts every transaction, without exception, through that trust layer,&quot; Hinchcliffe said. &quot;That&#x27;s best practice, in our view. If you don&#x27;t have a dedicated system checking policy compliance, toxicity, grounding, security, and privacy on every agentic activity, you can&#x27;t roll it out at scale.&quot;</p><p><a href=""https://www.bloomberg.com/profile/person/24214076"">Sameer Hasan</a>, who serves as Chief Technology and Digital Officer at Williams-Sonoma Inc., said the trust layer proved decisive in his company&#x27;s decision to adopt Agentforce across its portfolio of brands, which includes <a href=""https://www.potterybarn.com/"">Pottery Barn</a>, <a href=""https://www.westelm.com/"">West Elm</a>, and the flagship <a href=""https://www.williams-sonoma.com/"">Williams-Sonoma</a> stores that together serve approximately 20% of the U.S. home furnishings market.</p><p>&quot;The area that caused us to make sure—let&#x27;s be slow, let&#x27;s not move too fast, and let this get out of control—is really around security, privacy, and brand reputation,&quot; Hasan said. &quot;The minute you start to put this tech in front of customers, there&#x27;s the risk of what could happen if the AI says the wrong thing or does the wrong thing. There&#x27;s plenty of folks out there that are intentionally trying to get the AI to do the wrong thing.&quot;</p><p>Hasan noted that while the underlying large language models powering Agentforce — including technology from <a href=""https://openai.com/"">OpenAI</a> and <a href=""https://www.anthropic.com/"">Anthropic</a> — are broadly available, the enterprise governance infrastructure is not.</p><p>&quot;We all have access to that. You don&#x27;t need Agentforce to go build a chatbot,&quot; Hasan said. &quot;What Agentforce helped us do more quickly and with more confidence is build something that&#x27;s more enterprise-ready. So there&#x27;s toxicity detection, the way that we handle PII and PII tokenization, data security and creating specific firewalls and separations between the generative tech and the functional tech, so that the AI doesn&#x27;t have the ability to just go comb through all of our customer and order data.&quot;</p><p>The trust concerns appear well-founded. The Information reported that among Salesforce&#x27;s own executives, <a href=""https://www.theinformation.com/articles/salesforce-executives-say-trust-generative-ai-declined"">trust in generative AI has actually declined</a> — an acknowledgment that even insiders recognize the technology requires careful deployment.</p><h2><b>Corporate travel startup Engine deployed an AI agent in 12 days and saved $2 million</b></h2><p>For <a href=""https://engine.com/b"">Engine</a>, a corporate travel platform valued at <a href=""https://www.reuters.com/technology/travel-tech-startup-hotel-engine-valued-21-bln-after-latest-fundraise-2024-09-17/"">$2.1 billion </a>following its Series C funding round, the business case for Agentforce crystallized around a specific customer pain point: cancellations.</p><p><a href=""https://www.salesforce.com/customer-stories/engine/"">Demetri Salvaggio</a>, Engine&#x27;s Vice President of Customer Experience and Operations, said his team analyzed customer support data and discovered that cancellation requests through chat channels represented a significant volume of contacts — work that required human agents but followed predictable patterns.</p><p>Engine deployed its first AI agent, named Eva, in just 12 business days. The speed surprised even Salvaggio, though he acknowledged that Engine&#x27;s existing integration with Salesforce&#x27;s broader platform provided a foundation that accelerated implementation.</p><p>&quot;We saw success right away,&quot; Salvaggio said. &quot;But we went through growing pains, too. Early on, there wasn&#x27;t the observability you&#x27;d want at your fingertips, so we were doing a lot of manual work.&quot;</p><p>Those early limitations have since been addressed through <a href=""https://www.salesforce.com/form/conf/agentforce-demos/"">Salesforce&#x27;s Agentforce Studio</a>, which now provides real-time analytics showing exactly where AI agents struggle with customer questions — data that allows companies to continuously refine agent behavior.</p><p>The business results, according to Salvaggio, have been substantial. Engine reports approximately $2 million in annual cost savings attributable to Eva, alongside a customer satisfaction score improvement from 3.7 to 4.2 on a five-point scale — an increase Salvaggio described as &quot;really cool to see.&quot;</p><p>&quot;Our current numbers show $2 million in cost savings that she&#x27;s able to address for us,&quot; Salvaggio said. &quot;We&#x27;ve seen CSAT go up with Eva. We&#x27;ve been able to go from like a 3.7 out of five scale to 4.2. We&#x27;ve had some moments at 85%.&quot;</p><p>Perhaps more telling than the cost savings is Engine&#x27;s philosophy around AI deployment. Rather than viewing Agentforce as a headcount-reduction tool, Salvaggio said the company focuses on productivity and customer experience improvements.</p><p>&quot;When you hear some companies talk about AI, it&#x27;s all about, &#x27;How do I get rid of all my employees?&#x27;&quot; Salvaggio said. &quot;Our approach is different. If we can avoid adding headcount, that&#x27;s a win. But we&#x27;re really focused on how to create a better customer experience.&quot;</p><p>Engine has since expanded beyond its initial cancellation use case. The company now operates multiple AI agents — including IT, HR, product, and finance assistants deployed through Slack — that Salvaggio collectively refers to as &quot;multi-purpose admin&quot; agents.</p><h2><b>Williams-Sonoma is using AI agents to recreate the in-store shopping experience online</b></h2><p>Williams-Sonoma&#x27;s AI deployment illustrates a more ambitious vision: using AI agents not merely to reduce costs but to fundamentally reimagine how customers interact with brands digitally.</p><p>Hasan described a frustration that anyone who has used e-commerce over the past two decades will recognize. Traditional chatbots feel robotic, impersonal, and limited — good at answering simple questions but incapable of the nuanced guidance a knowledgeable store associate might provide.</p><p>&quot;We&#x27;ve all had experiences with chatbots, and more often than not, they&#x27;re not positive,&quot; Hasan said. &quot;Historically, chatbot capabilities have been pretty basic. But when customers come to us with a service question, it&#x27;s rarely that simple — &#x27;Where&#x27;s my order?&#x27; &#x27;It&#x27;s here.&#x27; &#x27;Great, thanks.&#x27; It&#x27;s far more nuanced and complex.&quot;</p><p>Williams-Sonoma&#x27;s AI agent, called Olive, goes beyond answering questions to actively engaging customers in conversations about entertaining, cooking, and lifestyle — the same consultative approach the company&#x27;s in-store associates have provided for decades.</p><p>&quot;What separates our brands from others in the industry—and certainly from the marketplaces—is that we&#x27;re not just here to sell you a product,&quot; Hasan said. &quot;We&#x27;re here to help you, educate you, elevate your life. With Olive, we can connect the dots.&quot;</p><p>The agent draws on Williams-Sonoma&#x27;s proprietary recipe database, product expertise, and customer data to provide personalized recommendations. A customer planning a dinner party might receive not just product suggestions but complete menu ideas, cooking techniques, and entertaining tips.</p><p>Thattai, the Salesforce AI executive, said Williams-Sonoma is in what he describes as the second stage of agentic AI maturity. The first stage involves simple question-and-answer interactions. The second involves agents that actually execute business processes. The third — which he said is the largest untapped opportunity — involves agents working proactively in the background.</p><p>Critically, Hasan said Williams-Sonoma does not attempt to disguise its AI agents as human. Customers know they&#x27;re interacting with AI.</p><p>&quot;We don&#x27;t try to hide it,&quot; Hasan said. &quot;We know customers may come in with preconceptions. I&#x27;m sure plenty of people are rolling their eyes thinking, &#x27;I have to deal with this AI thing&#x27;—because their experience with other companies has been that it&#x27;s a cost-cutting maneuver that creates friction.&quot;</p><p>The company surveys customers after AI interactions and benchmarks satisfaction against human-assisted interactions. According to Hasan, the AI now matches human benchmarks — a constraint the company refuses to compromise.</p><p>&quot;We have a high bar for service—a white-glove customer experience,&quot; Hasan said. &quot;AI has to at least maintain that bar. If anything, our goal is to raise it.&quot;</p><p><a href=""https://www.williams-sonoma.com/"">Williams-Sonoma</a> moved from pilot to full production in 28 days, according to Salesforce — a timeline that Thattai said demonstrates how quickly companies can deploy when they build on existing platform infrastructure rather than starting from scratch.</p><h2><b>The three stages of enterprise AI maturity that determine whether companies see ROI</b></h2><p>Beyond the headline customer statistics, Thattai outlined a three-stage maturity framework that he said describes how most enterprises approach agentic AI:</p><p>Stage one involves building simple agents that answer questions — essentially sophisticated chatbots that can access company data to provide accurate, contextual responses. The primary challenge at this stage is ensuring the agent has comprehensive access to relevant information.</p><p>Stage two involves agents that execute workflows — not just answering &quot;what time does my flight leave?&quot; but actually rebooking a flight when a customer asks. Thattai cited Adecco, the recruiting company, as an example of stage-two deployment. The company uses Agentforce to qualify job candidates and match them with roles — a process that involves roughly 30 discrete steps, conditional decisions, and interactions with multiple systems.</p><p>&quot;A large language model by itself can&#x27;t execute a process that complex, because some steps are deterministic and need to run with certainty,&quot; Thattai explained. &quot;Our hybrid reasoning engine uses LLMs for decision-making and reasoning, while ensuring the deterministic steps execute with precision.&quot;</p><p>Stage three — and the one Thattai described as the largest future opportunity — involves agents working proactively in the background without customer initiation. He described a scenario in which a company might have thousands of sales leads sitting in a database, far more than human sales representatives could ever contact individually.</p><p>&quot;Most companies don&#x27;t have the bandwidth to reach out and qualify every one of those customers,&quot; Thattai said. &quot;But if you use an agent to refine profiles and personalize outreach, you&#x27;re creating incremental opportunities that humans simply don&#x27;t have the capacity for.&quot;</p><h2><b>Salesforce edges out Microsoft in analyst rankings of enterprise AI platforms</b></h2><p>The Futurum Group&#x27;s <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">recent analysis</a> of agentic AI platforms placed Salesforce at the top of its rankings, slightly ahead of Microsoft. The report evaluated ten major platforms — including offerings from <a href=""https://aws.amazon.com/"">AWS</a>, <a href=""https://www.google.com/"">Google</a>, <a href=""https://www.ibm.com/us-en"">IBM</a>, <a href=""https://www.oracle.com/"">Oracle</a>, <a href=""https://www.sap.com/index.html"">SAP</a>, <a href=""https://www.servicenow.com/"">ServiceNow</a>, and <a href=""https://www.uipath.com/"">UiPath</a> — across five dimensions: business value, product innovation, strategic vision, go-to-market execution, and ecosystem alignment.</p><p>Salesforce scored above 90 (out of 100) across all five categories, placing it in what the firm calls the &quot;Elite&quot; zone. Microsoft trailed closely behind, with both companies significantly outpacing competitors.</p><p>Thattai acknowledged the competitive pressure but argued that Salesforce&#x27;s existing position in customer relationship management provides structural advantages that pure-play AI companies cannot easily replicate.</p><p>&quot;The richest and most critical data a company has — data about their customers — lives within Salesforce,&quot; Thattai said. &quot;Most of our large customers use us for multiple functions: sales, service, and marketing. That complete view of the customer is central to running any business.&quot;</p><p>The platform advantage extends beyond data. Salesforce&#x27;s existing workflow infrastructure means that AI agents can immediately access business processes that have already been defined and refined — a head start that requires years for competitors to match.</p><p>&quot;Salesforce is not just a place where critical data is put, which it is, but it&#x27;s also where work is performed,&quot; Thattai said. &quot;The process by which a business runs happens in this application — how a sales process is managed, how a marketing process is managed, how a customer service process is managed.&quot;</p><h2><b>Why analysts say 2026 will be the real year of AI agents in the enterprise</b></h2><p>Despite the momentum, both Salesforce executives and independent analysts cautioned that <a href=""https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/blogs/pulse-check-series-latest-ai-developments/ai-adoption-challenges-ai-trends.html"">enterprise AI remains in early innings</a>.</p><p>Hinchcliffe pushed back against the notion that 2025 was &quot;the year of agents,&quot; a phrase that circulated widely at the beginning of the year.</p><p>&quot;This was not the year of agents,&quot; Hinchcliffe said. &quot;This was the year of finding out how ready they were, learning the platforms, and discovering where they weren&#x27;t mature yet. The biggest complaint we heard was that there&#x27;s no easy way to manage them. Once companies got all these agents running, they realized: I have to do lifecycle management. I have agents running on old versions, but their processes aren&#x27;t finished. How do I migrate them?&quot;</p><p>He predicted 2026 has &quot;a much more likely chance of being the year of agents,&quot; though added that the &quot;biggest year of agents&quot; is &quot;probably going to be the year after that.&quot;</p><p>The <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">Futurum Group&#x27;s analysis forecasts</a> the AI platform market growing from $127 billion in 2024 to $440 billion by 2029 — a compound annual growth rate that dwarfs most enterprise software categories.</p><p>For companies still on the sidelines, Salvaggio offered pointed advice based on Engine&#x27;s early-adopter experience.</p><p>&quot;Don&#x27;t take the fast-follower strategy with this technology,&quot; he said. &quot;It feels like it&#x27;s changing every week. There&#x27;s a differentiation period coming — if it hasn&#x27;t started already — and companies that waited are going to fall behind those that moved early.&quot;</p><p>He warned that institutional knowledge about AI deployment is becoming a competitive asset in itself — expertise that cannot be quickly acquired through outside consultants.</p><p>&quot;Companies need to start building AI expertise into their employee base,&quot; Salvaggio said. &quot;You can&#x27;t outsource all of this — you need that institutional knowledge within your organization.&quot;</p><p>Thattai struck a similarly forward-looking note, drawing parallels to previous platform shifts.</p><p>&quot;Think about the wave of mobile technology—apps that created entirely new ways of interacting with companies,&quot; he said. &quot;You&#x27;re going to see that happen with agentic technology. The difference is it will span every channel — voice, chat, mobile, web, text — all tied together by a personalized conversational experience.&quot;</p><p>The question for enterprises is no longer whether AI agents will transform customer and employee experiences. The data from Salesforce&#x27;s customer base suggests that transformation is already underway, generating measurable returns for early adopters willing to invest in platform infrastructure rather than waiting for a theoretical bubble to burst.</p><p>&quot;I feel incredibly confident that point solutions in each of those areas are not the path to getting to an agentic enterprise,&quot; Thattai said. &quot;The platform approach that we&#x27;ve taken to unlock all of this data in this context is really the way that customers are going to get value.&quot;</p>","artificial intelligence, generative AI, automation","structural, infrastructure, concrete"
Red teaming LLMs exposes a harsh truth about the AI security arms race,2025-12-22,VentureBeat,https://venturebeat.com/security/red-teaming-llms-harsh-truth-ai-security-arms-race,"<p>Unrelenting, persistent attacks on frontier models make them fail, with the patterns of failure varying by model and developer. Red teaming shows that it’s not the sophisticated, complex attacks that can bring a model down; it’s the attacker automating continuous, random attempts that will inevitably force a model to fail.</p><p>That’s the harsh truth that AI apps and platform builders need to plan for as they build each new release of their products. Betting an entire build-out on a frontier model prone to red team failures due to persistency alone is like building a house on sand. Even with red teaming, frontier LLMs, including those with open weights, are lagging behind adversarial and weaponized AI.</p><h2>The arms race has already started</h2><p><a href=""https://cybersecurityventures.com/cybercrime-damage-costs-10-trillion-by-2025/"">Cybercrime costs reached $9.5 trillion in 2024</a> and forecasts exceed $10.5 trillion for 2025. LLM vulnerabilities contribute to that trajectory. A financial services firm deploying a customer-facing LLM without adversarial testing saw it leak internal FAQ content within weeks. Remediation cost $3 million and triggered regulatory scrutiny. One enterprise software company had its entire salary database leaked after executives used an LLM for financial modeling, VentureBeat has learned.</p><p>The UK AISI/Gray Swan challenge ran 1.8 million attacks across 22 models. Every model broke. No current frontier system resists determined, well-resourced attacks.</p><p>Builders face a choice. Integrate security testing now, or explain breaches later. The tools exist — PyRIT, DeepTeam, Garak, OWASP frameworks. What remains is execution.</p><p>Organizations that treat LLM security as a feature rather than a foundation will learn the difference the hard way. The arms race rewards those who refuse to wait.</p><h2>Red teaming reflects how nascent frontier models are </h2><p>The gap between offensive capability and defensive readiness has never been wider. &quot;If you&#x27;ve got adversaries breaking out in two minutes, and it takes you a day to ingest data and another day to run a search, how can you possibly hope to keep up?&quot; <a href=""https://venturebeat.com/security/outsmarting-ai-powered-cyber-attacks-endpoint-defense-2025"">Elia Zaitsev, CTO of CrowdStrike</a>, told VentureBeat back in January. Zaitsev also implied that adversarial AI is progressing so quickly that the traditional tools AI builders trust to power their applications can be weaponized in stealth, jeopardizing product initiatives in the process.</p><p>Red teaming results to this point are a paradox, especially for AI builders who need a stable base platform to build from. Red teaming proves that every frontier model fails under sustained pressure. </p><p>One of my favorite things to do immediately after a new model comes out is to read the system card. It’s fascinating to see how well these documents reflect the red teaming, security, and reliability mentality of every model provider shipping today.</p><p>Earlier this month, I looked at how <a href=""https://venturebeat.com/security/anthropic-vs-openai-red-teaming-methods-reveal-different-security-priorities"">Anthropic’s versus OpenAI’s red teaming practices</a> reveal how different these two companies are when it comes to enterprise AI itself. That’s important for builders to know, as getting locked in on a platform that isn’t compatible with the building team’s priorities can be a massive waste of time.</p><h2>Attack surfaces are moving targets, further challenging red teams</h2><p>Builders need to understand how fluid the attack surfaces are that red teams attempt to cover, despite having incomplete knowledge of the many threats their models will face.</p><p>A good place to start is with one of the best-known frameworks. <a href=""https://genai.owasp.org/llm-top-10/"">OWASP&#x27;s 2025 Top 10 for LLM Applications</a> reads like a cautionary tale for any business building AI apps and attempting to expand on existing LLMs. Prompt injection sits at No. 1 for the second consecutive year. Sensitive information disclosure jumped from sixth to second place. Supply chain vulnerabilities climbed from fifth to third. These rankings reflect production incidents, not theoretical risks.</p><p>Five new vulnerability categories appeared in the 2025 list: excessive agency, system prompt leakage, vector and embedding weaknesses, misinformation, and unbounded consumption. Each represents a failure mode unique to generative AI systems. No one building AI apps can ignore these categories at the risk of shipping vulnerabilities that security teams never detected, or worse, lost track of given how mercurial threat surfaces can change.</p><p>&quot;AI is fundamentally changing everything, and cybersecurity is at the heart of it. We&#x27;re no longer dealing with human-scale threats; these attacks are occurring at machine scale,&quot; <a href=""https://venturebeat.com/security/rsac-2025-why-the-ai-agent-era-means-more-demand-for-cisos"">Jeetu Patel, Cisco&#x27;s President and Chief Product Officer, </a>emphasized to VentureBeat at RSAC 2025. Patel noted that AI-driven models are non-deterministic: &quot;They won&#x27;t give you the same answer every single time, introducing unprecedented risks.&quot;</p><p>&quot;We recognized that adversaries are increasingly leveraging AI to accelerate attacks. With Charlotte AI, we&#x27;re giving defenders an equal footing, amplifying their efficiency and ensuring they can keep pace with attackers in real-time,&quot; <a href=""https://venturebeat.com/security/crowdstrikes-ai-slashes-soc-workloads-over-40-hours-a-week"">Zaitsev told VentureBeat</a>.</p><h2>How and why model providers validate security differently</h2><p>Each frontier model provider wants to prove the security, robustness, and reliability of their system by devising a unique and differentiated red teaming process that is often explained in their system cards.</p><p>From their system cards, it doesn’t take long to see how different each model provider’s approach to red teaming reflects how different each is when it comes to security validation, versioning compatibility or the lack of it, persistence testing, and a willingness to torture-test their models with unrelenting attacks until they break.</p><p>In many ways, red teaming of frontier models is a lot like quality assurance on a commercial jet assembly line. Anthropic’s mentality is comparable to the well-known tests Airbus, Boeing, Gulfstream, and others do. Often called the Wing Bend Test or Ultimate Load Test, the goal of these tests is to push a wing’s strength to the breaking point to ensure the most significant safety margins possible. </p><p>Be sure to read <a href=""https://venturebeat.com/security/anthropic-vs-openai-red-teaming-methods-reveal-different-security-priorities"">Anthropic&#x27;s 153-page system card for Claude Opus 4.5 versus OpenAI&#x27;s 55-page GPT-5 system card</a> to see firsthand how different their measurement philosophies are. Anthropic relies on multi-attempt attack success rates from 200-attempt reinforcement learning campaigns. OpenAI reports single-attempt jailbreak resistance.</p><p><a href=""https://www.grayswan.ai/product/shade"">Gray Swan&#x27;s Shade platform</a> ran a series of adaptive adversarial campaigns against Claude models. The attack success rate (ASR) metrics quantify just how quickly models can degrade and break under pressure. Opus 4.5 in coding environments hit 4.7% ASR at one attempt, 33.6% at 10 attempts, and 63.0% at 100 attempts. In computer use with extended thinking, it held at 0% ASR even after 200 attempts, becoming the first model to saturate the benchmark. Sonnet 4.5 at the same thresholds showed 70% ASR in coding and 85.7% in computer use. The gap between model tiers within the same family is now larger than some cross-vendor comparisons.</p><p>On Gray Swan&#x27;s prompt injection benchmark, Claude Opus 4.5 shows the strongest resistance with an attack success rate of 4.7%, compared with 12.5% for Gemini 3 Pro and 21.9% for GPT-5.1. </p><p>For OpenAI&#x27;s o1, the system card reported 6% ASR for harmful text and 5% for malicious code based on single-attempt metrics. Third-party testing by SPLX and NeuralTrust found GPT-5&#x27;s raw ASR hit 89% initially, dropping below 1% within two weeks of patching. </p><p>The <a href=""https://arxiv.org/abs/2404.02151"">report confirms the pattern</a>: With sufficient attempts, Claude 3.5 Sonnet showed 78% ASR, and GPT-4o reached 89%.</p><h2>M<!-- -->odels attempt to game tests during red teaming — adding to the paradox </h2><p>Models scheme to turn off the oversight mechanisms when they predict they will be shut down. In Apollo Research&#x27;s evaluation of o1, the model attempted to turn off oversight mechanisms 5% of the time when it thought it would be shut down. In 2% of cases, o1 attempted self-exfiltration. When confronted about misaligned actions, o1 denied wrongdoing or fabricated explanations 99% of the time.</p><p>In 37% of scenarios, the model pursued developer-aligned goals during evaluation, then switched to its own goals when it believed oversight was minimal. Anthropic reports Opus 4.5&#x27;s evaluation awareness dropped from 26.5% (Opus 4.1) to less than 10% internally.</p><p><b>Evaluating Anthropic versus OpenAI red teaming results</b></p><table><tbody><tr><td><p><b>Dimension</b></p></td><td><p><b>Claude Opus 4.5</b></p></td><td><p><b>GPT-5</b></p></td><td><p><b>o1</b></p></td></tr><tr><td><p>System card length</p></td><td><p>153 pages</p></td><td><p>55 pages</p></td><td><p>~40 pages</p></td></tr><tr><td><p>Attack methodology</p></td><td><p>200-attempt RL campaigns</p></td><td><p>Single-attempt + patching</p></td><td><p>Single-attempt</p></td></tr><tr><td><p>ASR @ 1 attempt</p></td><td><p>4.7% (coding)</p></td><td><p>89% raw (pre-patch)</p></td><td><p>5-6%</p></td></tr><tr><td><p>ASR @ 200 attempts</p></td><td><p>0% (saturates benchmark)</p></td><td><p>Not published</p></td><td><p>Not published</p></td></tr><tr><td><p>Prompt injection defense</p></td><td><p>96-99.4% prevention</p></td><td><p>~20% ASR (Fujitsu)</p></td><td><p>Higher than GPT-4o</p></td></tr><tr><td><p>Interpretability</p></td><td><p>~10M neural features</p></td><td><p>CoT monitoring</p></td><td><p>CoT monitoring</p></td></tr><tr><td><p>Evaluation awareness</p></td><td><p>&lt;10%</p></td><td><p>Identifies exact eval</p></td><td><p>37% alignment faking</p></td></tr><tr><td><p>Scheming behavior</p></td><td><p>Orders of magnitude lower</p></td><td><p>Not published</p></td><td><p>5% oversight off; 2% self-exfil</p></td></tr></tbody></table><p><i>Sources: Opus 4.5 system card, GPT-5 system card, o1 system card, Gray Swan, METR, Apollo Research</i></p><p>When models attempt to game a red teaming exercise if they anticipate they’re about to be shut down, AI builders need to know the sequence that leads to that logic being created. No one wants a model resisting being shut down in an emergency or commanding a given production process or workflow.</p><h2>Defensive tools struggle against adaptive attackers</h2><p>&quot;Threat actors using AI as an attack vector has been accelerated, and they are so far in front of us as defenders, and we need to get on a bandwagon as defenders to start utilizing AI,&quot; Mike Riemer, Field CISO at Ivanti, told VentureBeat. </p><p>Riemer pointed to patch reverse-engineering as a concrete example of the speed gap: &quot;They&#x27;re able to reverse engineer a patch within 72 hours. So if I release a patch and a customer doesn&#x27;t patch within 72 hours of that release, they&#x27;re open to exploit because that&#x27;s how fast they can now do it,&quot; he noted in a <a href=""https://venturebeat.com/security/weaponized-ai-can-dismantle-patches-in-72-hours-but-ivantis-kernel-defense"">recent VentureBeat interview</a>.</p><p>An <a href=""https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/"">October 2025 paper from researchers</a> — including representatives from OpenAI, Anthropic, and Google DeepMind — examined 12 published defenses against prompt injection and jailbreaking. Using adaptive attacks that iteratively refined their approach, the researchers bypassed defenses with attack success rates above 90% for most. The majority of defenses had initially been reported to have near-zero attack success rates.</p><p>The gap between reported defense performance and real-world resilience stems from evaluation methodology. Defense authors test against fixed attack sets. Adaptive attackers are very aggressive in using iteration, which is a common theme in all attempts to compromise any model.</p><p>Builders shouldn’t rely on frontier model builders&#x27; claims without also conducting their own testing.</p><p>Open-source frameworks have emerged to address the testing gap. <a href=""https://www.helpnetsecurity.com/2025/11/26/deepteam-open-source-llm-red-teaming-framework/"">DeepTeam, released in November 2025</a>, applies jailbreaking and prompt injection techniques to probe LLM systems before deployment. Garak from Nvidia focuses on vulnerability scanning. MLCommons published safety benchmarks. The tooling ecosystem is maturing, but builder adoption lags behind attacker sophistication.</p><h2>What AI builders need to do now</h2><p>&quot;An AI agent is like giving an intern full access to your network. You gotta put some guardrails around the intern.&quot; <a href=""https://venturebeat.com/security/soc-teams-face-51-second-breach-reality-manual-response-times-are-officially"">George Kurtz, CEO and founder of CrowdStrike, observed at FalCon 2025</a>. That quote typifies the current state of frontier AI models as well. </p><p><a href=""https://ai.meta.com/blog/practical-ai-agent-security/"">Meta&#x27;s Agents Rule of Two</a>, published October 2025, reinforces this principle: Guardrails must live outside the LLM. File-type firewalls, human approvals, and kill switches for tool calls cannot depend on model behavior alone. Builders who embed security logic inside prompts have already lost.</p><p>&quot;Business and technology leaders can&#x27;t afford to sacrifice safety for speed when embracing AI. The security challenges AI introduces are new and complex, with vulnerabilities spanning models, applications, and supply chains. We have to think differently,&quot; <a href=""https://venturebeat.com/security/how-ciscos-ai-defense-stacks-up-against-the-cyber-threats-you-never-see"">Patel told VentureBeat</a> previously.</p><ul><li><p><b>Input validation remains the first line of defense.</b> Enforce strict schemas that define exactly what inputs the LLM endpoints being designed can accept. Reject unexpected characters, escape sequences, and encoding variations. Apply rate limits per user and per session. Create structured interfaces or prompt templates that limit free-form text injection into sensitive contexts.</p></li><li><p><b>Output validation from any LLM or frontier model is a must-have.</b> LLM-generated content passed to downstream systems without sanitization creates classic injection risks: XSS, SQL injection, SSRF, and remote code execution. Treat the model as an untrusted user. Follow OWASP ASVS guidelines for input validation and sanitization.</p></li><li><p><b>Always separate instructions from data.</b> Use different input fields for system instructions and dynamic user content. Prevent user-provided content from being embedded directly into control prompts. This architectural decision prevents entire classes of injection attacks.</p></li><li><p><b>Think of regular red teaming as the muscle memory you always needed; it’s that essential.</b> The <a href=""https://genai.owasp.org/2024/09/12/research-initiative-ai-red-teaming-evaluation/"">OWASP Gen AI Red Teaming Guide</a> provides structured methodologies for identifying model-level and system-level vulnerabilities. Quarterly adversarial testing should become standard practice for any team shipping LLM-powered features.</p></li><li><p><b>Control agent permissions ruthlessly.</b> For LLM-powered agents that can take actions, minimize extensions and their functionality. Avoid open-ended extensions. Execute extensions in the user&#x27;s context with their permissions. Require user approval for high-impact actions. The principle of least privilege applies to AI agents just as it applies to human users.</p></li><li><p><b>Supply chain scrutiny cannot wait.</b> Vet data and model sources. Maintain a software bill of materials for AI components using tools like OWASP CycloneDX or ML-BOM. Run custom evaluations when selecting third-party models rather than relying solely on public benchmarks.</p></li></ul><p></p>",generative AI,concrete
Agent autonomy without guardrails is an SRE nightmare,2025-12-21,VentureBeat,https://venturebeat.com/infrastructure/agent-autonomy-without-guardrails-is-an-sre-nightmare,"<p><i>João Freitas is GM and VP of engineering for AI and automation at </i><a href=""https://www.pagerduty.com/""><i>PagerDuty</i></a></p><p>As AI use continues to evolve in large organizations, leaders are increasingly seeking the next development that will yield major ROI. The latest wave of this ongoing trend is the adoption of AI agents. However, as with any new technology, organizations must ensure they adopt AI agents in a responsible way that allows them to facilitate both speed and security. </p><p><a href=""https://venturebeat.com/ai/hiring-specialists-made-sense-before-ai-now-generalists-win"">More than half</a> of organizations have already deployed AI agents to some extent, with more expecting to follow suit in the next two years. But many early adopters are now reevaluating their approach. Four-in-10 tech leaders regret not establishing a <a href=""https://venturebeat.com/ai/why-observable-ai-is-the-missing-sre-layer-enterprises-need-for-reliable"">stronger governance foundation</a> from the start, which suggests they adopted AI rapidly, but with margin to improve on policies, rules and best practices designed to ensure the responsible, ethical and legal development and use of AI.</p><p><a href=""https://venturebeat.com/ai/hiring-specialists-made-sense-before-ai-now-generalists-win"">As AI adoption accelerates</a>, organizations must find the right balance between their exposure risk and the implementation of guardrails to ensure AI use is secure.</p><h2><b>Where do AI agents create potential risks?</b></h2><p>There are three principal areas of consideration for safer AI adoption.</p><p>The first is <a href=""https://venturebeat.com/security/shadow-ai-doubles-every-18-months-creating-blind-spots-socs-never-see"">shadow AI</a>, when employees use unauthorized AI tools without express permission, bypassing approved tools and processes. IT should create necessary processes for experimentation and innovation to introduce more efficient ways of working with AI. While shadow AI has existed as long as AI tools themselves, AI agent autonomy makes it easier for unsanctioned tools to operate outside the purview of IT, which can introduce fresh security risks.</p><p>Secondly, organizations must close gaps in AI ownership and accountability to prepare for incidents or processes gone wrong. The strength of AI agents lies in their autonomy. However, if agents act in unexpected ways, teams must be able to determine who is responsible for addressing any issues.</p><p>The third risk arises when there is a lack of explainability for actions AI agents have taken. <a href=""https://venturebeat.com/ai/ontology-is-the-real-guardrail-how-to-stop-ai-agents-from-misunderstanding"">AI agents are goal-oriented</a>, but how they accomplish their goals can be unclear. AI agents must have explainable logic underlying their actions so that engineers can trace and, if needed, roll back actions that may cause issues with existing systems.</p><p>While none of these risks should delay adoption, they will help organizations better ensure their security.</p><h2><b>The three guidelines for responsible AI agent adoption</b></h2><p>Once organizations have identified the risks AI agents can pose, they must implement guidelines and guardrails to ensure safe usage. By following these three steps, organizations can minimize these risks.</p><p><b>1: Make human oversight the default </b></p><p>AI agency continues to evolve at a fast pace. However, we still need human oversight when AI agents are given the  capacity to act, make decisions and pursue a goal that may impact key systems. A human should be in the loop by default, especially for business-critical use cases and systems. The teams that use AI must understand the actions it may take and where they may need to intervene. Start conservatively and, over time, increase the level of agency given to AI agents.</p><p>In conjunction, operations teams, engineers and security professionals must understand the role they play in supervising AI agents’ workflows. Each agent should be assigned a specific human owner for clearly defined oversight and accountability. Organizations must also allow any human to flag or override an AI agent’s behavior when an action has a negative outcome.</p><p>When considering tasks for AI agents, organizations should understand that, while traditional automation is good at handling repetitive, rule-based processes with structured data inputs, AI agents can handle much more complex tasks and adapt to new information in a more autonomous way. This makes them an appealing solution for all sorts of tasks. But as AI agents are deployed, organizations should control what actions the agents can take, particularly in the early stages of a project. Thus, teams working with AI agents should have approval paths in place for high-impact actions to ensure agent scope does not extend beyond expected use cases, minimizing risk to the wider system.</p><p><b>2: Bake in security </b></p><p>The introduction of new tools should not expose a system to fresh security risks. </p><p>Organizations should consider agentic platforms that comply with high security standards and are validated by enterprise-grade certifications such as SOC2, FedRAMP or equivalent. Further, AI agents should not be allowed free rein across an organization’s systems. At a minimum, the permissions and security scope of an AI agent must be aligned with the scope of the owner, and any tools added to the agent should not allow for extended permissions. Limiting AI agent access to a system based on their role will also ensure deployment runs smoothly. Keeping complete logs of every action taken by an AI agent can also help engineers understand what happened in the event of an incident and trace back the problem.</p><p><b>3: Make outputs explainable </b></p><p>AI use in an organization must never be a black box. The reasoning behind any action must be illustrated so that any engineer who tries to access it can understand the context the agent used for decision-making and access the traces that led to those actions.</p><p>I<!-- -->nputs and outputs for every action should be logged and accessible. This will help organizations establish a firm overview of the logic underlying an AI agent’s actions, providing significant value in the event anything goes wrong.</p><h2><b>Security underscores AI agents’ success</b></h2><p>AI agents offer a huge opportunity for organizations to accelerate and improve their existing processes. However, if they do not prioritize security and strong governance, they could expose themselves to new risks. </p><p>As AI agents become more common, organizations must ensure they have systems in place to measure how they perform and the ability to take action when they create problems.</p><p><i>Read more from our </i><a href=""https://venturebeat.com/datadecisionmakers""><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=""https://venturebeat.com/guest-posts""><i>guidelines here</i></a><i>. </i></p>",automation,infrastructure
