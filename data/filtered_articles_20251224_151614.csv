title,date,source,summary,url
"Elon Musk, AI and the antichrist: the biggest tech stories of 2025","Tue, 23 Dec 2025 17:03:50 GMT",GuardianTech,"<p>A look back at the biggest tech stories of the year, from the rise and fall of Musk’s Doge to lucrative investments into AI</p><p>Hello, and welcome to TechScape. I’m your host, Blake Montgomery, wishing you a happy and healthy end of the year. I myself have a cold.</p><p>Today, we are looking back at the biggest stories in tech of 2025 – Elon Musk’s political rise, burst and fall; artificial intelligence’s subsumption of the global economy, all other technology, and even the Earth’s topography; Australia’s remarkable social media ban; the tech industry’s new Trumpian politics; and, as a treat, a glimpse of the apocalypse offered by one of Silicon Valley’s savviest and strangest billionaires.</p><p><a href=""https://www.theguardian.com/technology/2025/mar/15/elon-musk-18f-x-false-claims"">How an obscure US government office has become a target of Elon Musk</a></p><p><a href=""https://www.theguardian.com/technology/ng-interactive/2025/aug/28/elon-musk-antonio-gracias-mdma-psychedelics-company"">How Elon Musk’s billionaire Doge lieutenant took over the US’s biggest MDMA company | Technology | The Guardian</a></p><p><a href=""https://www.theguardian.com/technology/2025/may/29/elon-musk-trump-doge-adviser-exit"">The chaos Elon Musk and Doge are leaving behind in Washington</a></p><p><a href=""https://www.theguardian.com/technology/2025/mar/15/vandalized-tesla-elon-musk-trump"">Eggings, swastikas and dog poop: Tesla bears brunt of people’s ire against Musk</a></p><p><a href=""https://www.theguardian.com/technology/ng-interactive/2025/mar/02/tesla-owners-selling-musk"">‘I’m selling the Nazi mobile’: Tesla owners offload cars after Musk’s fascist-style salutes</a></p><p><a href=""https://www.theguardian.com/technology/2025/jul/17/hawaii-elon-musk-spacex-rocket-debris"">Inside Elon Musk’s plan to rain SpaceX’s rocket debris over Hawaii’s pristine waters</a></p><p><a href=""https://www.theguardian.com/science/2025/dec/10/elon-musk-spacex-preparing-for-2026-flotation-of-more-than-1tn"">Elon Musk’s SpaceX ‘preparing for flotation that could value it at over $1tn’</a></p> <a href=""https://www.theguardian.com/technology/2025/dec/22/biggest-tech-stories-2025"">Continue reading...</a>",https://www.theguardian.com/technology/2025/dec/22/biggest-tech-stories-2025
"When the AI bubble bursts, humans will finally have their chance to take back control | Rafael Behr","Tue, 23 Dec 2025 06:00:49 GMT",GuardianTech,"<p>The US economy is pumped up on tech-bro vanity. The inevitable correction must prompt a global conversation about intelligent machines, regulation and risk</p><p>If AI did not change your life in 2025, next year it will. That is one of few forecasts that can be made with confidence in unpredictable times. This is not an invitation to believe the hype about what the technology can do today, or may one day achieve. The hype doesn’t need your credence. It is <a href=""https://www.theguardian.com/technology/2025/nov/02/global-datacentre-boom-investment-debt"">puffed up enough</a> on Silicon Valley finance to distort the global economy and fuel geopolitical rivalries, shaping your world regardless of whether the most fanciful claims about AI capability are ever realised.</p><p>ChatGPT <a href=""https://www.theguardian.com/commentisfree/2022/dec/08/the-guardian-view-on-chatgpt-an-eerily-good-human-impersonator"">was launched</a> just over three years ago and became the fastest-growing consumer app in history. Now it has about 800m weekly users. Its parent company, OpenAI, is valued at <a href=""https://www.reuters.com/technology/openai-hits-500-billion-valuation-after-share-sale-source-says-2025-10-02/"">about $500bn</a>. Sam Altman, OpenAI CEO, has negotiated an intricate and, to some eyes, suspiciously opaque network of deals with other players in the sector to build the infrastructure required for the US’s AI-powered future. The value of these commitments is <a href=""https://www.reuters.com/sustainability/land-use-biodiversity/altman-touts-trillion-dollar-ai-vision-openai-restructures-chase-scale-2025-10-29/"">about $1.5tn</a>. This is not real cash, but bear in mind that a person spending $1 every <em>second</em> would need 31,700 years to get through a trillion-dollar stash.</p><p>Rafael Behr is a Guardian columnist</p> <a href=""https://www.theguardian.com/commentisfree/2025/dec/23/artificial-intelligence-ai-bubble-bursts-humans-take-back-control"">Continue reading...</a>",https://www.theguardian.com/commentisfree/2025/dec/23/artificial-intelligence-ai-bubble-bursts-humans-take-back-control
Extremists are using AI voice cloning to supercharge propaganda. Experts say it’s helping them grow,"Sun, 21 Dec 2025 13:00:01 GMT",GuardianTech,"<p>Researchers warn generative tools are helping militant groups from neo-Nazis to the Islamic State spread ideology</p><p>While the <a href=""https://www.theguardian.com/technology/artificialintelligenceai"">artificial intelligence</a> boom is <a href=""https://www.theguardian.com/technology/2025/jul/14/an-ai-generated-band-got-1m-plays-on-spotify-now-music-insiders-say-listeners-should-be-warned"">upending sections of the music industry</a>, voice generating bots are also becoming a boon to another unlikely corner of the internet: extremist movements that are using them to recreate the voices and speeches of major figures in their milieu, and experts say it is helping them grow.</p><p>“The adoption of AI-enabled translation by terrorists and extremists marks a significant evolution in digital propaganda strategies,” said Lucas Webber, a senior threat intelligence analyst at Tech Against Terrorism and a research fellow at the Soufan Center. Webber specializes in monitoring the online tools of terrorist groups and extremists around the world.</p> <a href=""https://www.theguardian.com/technology/2025/dec/21/ai-voice-cloning-nazis-islamic-state-extremism"">Continue reading...</a>",https://www.theguardian.com/technology/2025/dec/21/ai-voice-cloning-nazis-islamic-state-extremism
"While everyone talks about an AI bubble, Salesforce quietly added 6,000 enterprise customers in 3 months","Mon, 22 Dec 2025 14:00:00 GMT",VentureBeat,"<p>While Silicon Valley debates whether artificial intelligence has become an <a href=""https://www.reuters.com/business/finance/opinions-split-over-ai-bubble-after-billions-invested-2025-10-16/"">overinflated bubble</a>, Salesforce&#x27;s enterprise AI platform quietly added 6,000 new customers in a single quarter — a 48% increase that executives say demonstrates a widening gap between speculative AI hype and deployed enterprise solutions generating measurable returns.</p><p><a href=""https://www.salesforce.com/agentforce/"">Agentforce</a>, the company&#x27;s autonomous AI agent platform, now serves 18,500 enterprise customers, up from 12,500 the prior quarter. Those customers collectively run more than three billion automated workflows monthly and have pushed Salesforce&#x27;s agentic product revenue past $540 million in annual recurring revenue, according to figures the company shared with VentureBeat. The platform has processed over three trillion tokens — the fundamental units that large language models use to understand and generate text — positioning Salesforce as one of the largest consumers of AI compute in the enterprise software market.</p><p>&quot;This has been a year of momentum,&quot; Madhav Thattai, Salesforce&#x27;s Chief Operating Officer for AI, said in an exclusive interview with VentureBeat. &quot;We crossed over half a billion in ARR for our agentic products, which have been out for a couple of years. And so that&#x27;s pretty remarkable for enterprise software.&quot;</p><p>The numbers arrive amid intensifying scrutiny of AI spending across corporate America. Venture capitalists and analysts have questioned whether the <a href=""https://www.wsj.com/tech/it-really-is-possible-to-spend-too-much-on-ai-7bb68df1"">billions pouring into AI infrastructure</a> — from data centers to graphics processing units to model development — will ever generate proportionate returns. <a href=""https://ai.meta.com/"">Meta</a>, <a href=""https://www.microsoft.com/en-us/"">Microsoft</a>, and <a href=""https://www.amazon.com/"">Amazon</a> have committed tens of billions to AI infrastructure, prompting some investors to ask whether the enthusiasm has outpaced the economics.</p><p>Yet the <a href=""https://www.salesforce.com/"">Salesforce</a> data suggests that at least one segment of the AI market — enterprise workflow automation — is translating investments into concrete business outcomes at a pace that defies the bubble narrative.</p><h2><b>Why enterprise AI trust has become the defining challenge for CIOs in 2025</b></h2><p>The distinction between AI experimentation and AI deployment at scale comes down to one word that appeared repeatedly across interviews with Salesforce executives, customers, and independent analysts: trust.</p><p><a href=""http://m"">Dion Hinchcliffe</a>, who leads the CIO practice at technology research firm <a href=""https://futurumgroup.com/"">The Futurum Group</a>, said the urgency around enterprise AI has reached a fever pitch not seen in previous technology cycles. His firm recently completed a <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">comprehensive analysis of agentic AI platforms</a> that ranked Salesforce slightly ahead of Microsoft as the market leader.</p><p>&quot;I&#x27;ve been through revolution after revolution in this business,&quot; Hinchcliffe said. &quot;I&#x27;ve never seen anything like this before. In my entire career, I&#x27;ve never seen this level of business focus—boards of directors are directly involved, saying this is existential for the company.&quot;</p><p>The pressure flows downward. CIOs who once managed technology as a cost center now field questions directly from board members demanding to know how their companies will avoid being disrupted by AI-native competitors.</p><p>&quot;They&#x27;re pushing the CIO hard, asking, &#x27;What are we doing? How do we make sure we&#x27;re not put out of business by the next AI-first company that reimagines what we do?&#x27;&quot; Hinchcliffe said.</p><p>But that pressure creates a paradox. Companies want to move fast on AI, yet the very autonomy that makes AI agents valuable also makes them dangerous. An agent that can independently execute workflows, process customer data, and make decisions without human intervention can also make mistakes at machine speed — or worse, be manipulated by bad actors.</p><p>This is where enterprise AI platforms differentiate themselves from the consumer AI tools that dominate headlines. According to Hinchcliffe, building a production-grade agentic AI system requires hundreds of specialized engineers working on governance, security, testing, and orchestration — infrastructure that most companies cannot afford to build themselves.</p><p>&quot;The average enterprise-grade agentic team is 200-plus people working on an agentic platform,&quot; Hinchcliffe said. &quot;Salesforce has over 450 people working on agent AI.&quot;</p><p>Early in the AI adoption cycle, many CIOs attempted to build their own agent platforms using open-source tools like <a href=""https://www.langchain.com/"">LangChain</a>. They quickly discovered the complexity exceeded their resources.</p><p>&quot;They very quickly realized this problem was much bigger than expected,&quot; Hinchcliffe explained. &quot;To deploy agents at scale, you need infrastructure to manage them, develop them, test them, put guardrails on them, and govern them — because you&#x27;re going to have tens of thousands, hundreds of thousands, even millions of long-running processes out there doing work.&quot;</p><h2><b>How AI guardrails and security layers separate enterprise platforms from consumer chatbots</b></h2><p>The technical architecture that separates enterprise AI platforms from consumer tools centers on what the industry calls a &quot;trust layer&quot; — a set of software systems that monitor, filter, and verify every action an AI agent attempts to take.</p><p>Hinchcliffe&#x27;s research found that <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">only about half of the agentic AI platforms</a> his firm evaluated included runtime trust verification — the practice of checking every transaction for policy compliance, data toxicity, and security violations as it happens, rather than relying solely on design-time constraints that can be circumvented.</p><p>&quot;Salesforce puts every transaction, without exception, through that trust layer,&quot; Hinchcliffe said. &quot;That&#x27;s best practice, in our view. If you don&#x27;t have a dedicated system checking policy compliance, toxicity, grounding, security, and privacy on every agentic activity, you can&#x27;t roll it out at scale.&quot;</p><p><a href=""https://www.bloomberg.com/profile/person/24214076"">Sameer Hasan</a>, who serves as Chief Technology and Digital Officer at Williams-Sonoma Inc., said the trust layer proved decisive in his company&#x27;s decision to adopt Agentforce across its portfolio of brands, which includes <a href=""https://www.potterybarn.com/"">Pottery Barn</a>, <a href=""https://www.westelm.com/"">West Elm</a>, and the flagship <a href=""https://www.williams-sonoma.com/"">Williams-Sonoma</a> stores that together serve approximately 20% of the U.S. home furnishings market.</p><p>&quot;The area that caused us to make sure—let&#x27;s be slow, let&#x27;s not move too fast, and let this get out of control—is really around security, privacy, and brand reputation,&quot; Hasan said. &quot;The minute you start to put this tech in front of customers, there&#x27;s the risk of what could happen if the AI says the wrong thing or does the wrong thing. There&#x27;s plenty of folks out there that are intentionally trying to get the AI to do the wrong thing.&quot;</p><p>Hasan noted that while the underlying large language models powering Agentforce — including technology from <a href=""https://openai.com/"">OpenAI</a> and <a href=""https://www.anthropic.com/"">Anthropic</a> — are broadly available, the enterprise governance infrastructure is not.</p><p>&quot;We all have access to that. You don&#x27;t need Agentforce to go build a chatbot,&quot; Hasan said. &quot;What Agentforce helped us do more quickly and with more confidence is build something that&#x27;s more enterprise-ready. So there&#x27;s toxicity detection, the way that we handle PII and PII tokenization, data security and creating specific firewalls and separations between the generative tech and the functional tech, so that the AI doesn&#x27;t have the ability to just go comb through all of our customer and order data.&quot;</p><p>The trust concerns appear well-founded. The Information reported that among Salesforce&#x27;s own executives, <a href=""https://www.theinformation.com/articles/salesforce-executives-say-trust-generative-ai-declined"">trust in generative AI has actually declined</a> — an acknowledgment that even insiders recognize the technology requires careful deployment.</p><h2><b>Corporate travel startup Engine deployed an AI agent in 12 days and saved $2 million</b></h2><p>For <a href=""https://engine.com/b"">Engine</a>, a corporate travel platform valued at <a href=""https://www.reuters.com/technology/travel-tech-startup-hotel-engine-valued-21-bln-after-latest-fundraise-2024-09-17/"">$2.1 billion </a>following its Series C funding round, the business case for Agentforce crystallized around a specific customer pain point: cancellations.</p><p><a href=""https://www.salesforce.com/customer-stories/engine/"">Demetri Salvaggio</a>, Engine&#x27;s Vice President of Customer Experience and Operations, said his team analyzed customer support data and discovered that cancellation requests through chat channels represented a significant volume of contacts — work that required human agents but followed predictable patterns.</p><p>Engine deployed its first AI agent, named Eva, in just 12 business days. The speed surprised even Salvaggio, though he acknowledged that Engine&#x27;s existing integration with Salesforce&#x27;s broader platform provided a foundation that accelerated implementation.</p><p>&quot;We saw success right away,&quot; Salvaggio said. &quot;But we went through growing pains, too. Early on, there wasn&#x27;t the observability you&#x27;d want at your fingertips, so we were doing a lot of manual work.&quot;</p><p>Those early limitations have since been addressed through <a href=""https://www.salesforce.com/form/conf/agentforce-demos/"">Salesforce&#x27;s Agentforce Studio</a>, which now provides real-time analytics showing exactly where AI agents struggle with customer questions — data that allows companies to continuously refine agent behavior.</p><p>The business results, according to Salvaggio, have been substantial. Engine reports approximately $2 million in annual cost savings attributable to Eva, alongside a customer satisfaction score improvement from 3.7 to 4.2 on a five-point scale — an increase Salvaggio described as &quot;really cool to see.&quot;</p><p>&quot;Our current numbers show $2 million in cost savings that she&#x27;s able to address for us,&quot; Salvaggio said. &quot;We&#x27;ve seen CSAT go up with Eva. We&#x27;ve been able to go from like a 3.7 out of five scale to 4.2. We&#x27;ve had some moments at 85%.&quot;</p><p>Perhaps more telling than the cost savings is Engine&#x27;s philosophy around AI deployment. Rather than viewing Agentforce as a headcount-reduction tool, Salvaggio said the company focuses on productivity and customer experience improvements.</p><p>&quot;When you hear some companies talk about AI, it&#x27;s all about, &#x27;How do I get rid of all my employees?&#x27;&quot; Salvaggio said. &quot;Our approach is different. If we can avoid adding headcount, that&#x27;s a win. But we&#x27;re really focused on how to create a better customer experience.&quot;</p><p>Engine has since expanded beyond its initial cancellation use case. The company now operates multiple AI agents — including IT, HR, product, and finance assistants deployed through Slack — that Salvaggio collectively refers to as &quot;multi-purpose admin&quot; agents.</p><h2><b>Williams-Sonoma is using AI agents to recreate the in-store shopping experience online</b></h2><p>Williams-Sonoma&#x27;s AI deployment illustrates a more ambitious vision: using AI agents not merely to reduce costs but to fundamentally reimagine how customers interact with brands digitally.</p><p>Hasan described a frustration that anyone who has used e-commerce over the past two decades will recognize. Traditional chatbots feel robotic, impersonal, and limited — good at answering simple questions but incapable of the nuanced guidance a knowledgeable store associate might provide.</p><p>&quot;We&#x27;ve all had experiences with chatbots, and more often than not, they&#x27;re not positive,&quot; Hasan said. &quot;Historically, chatbot capabilities have been pretty basic. But when customers come to us with a service question, it&#x27;s rarely that simple — &#x27;Where&#x27;s my order?&#x27; &#x27;It&#x27;s here.&#x27; &#x27;Great, thanks.&#x27; It&#x27;s far more nuanced and complex.&quot;</p><p>Williams-Sonoma&#x27;s AI agent, called Olive, goes beyond answering questions to actively engaging customers in conversations about entertaining, cooking, and lifestyle — the same consultative approach the company&#x27;s in-store associates have provided for decades.</p><p>&quot;What separates our brands from others in the industry—and certainly from the marketplaces—is that we&#x27;re not just here to sell you a product,&quot; Hasan said. &quot;We&#x27;re here to help you, educate you, elevate your life. With Olive, we can connect the dots.&quot;</p><p>The agent draws on Williams-Sonoma&#x27;s proprietary recipe database, product expertise, and customer data to provide personalized recommendations. A customer planning a dinner party might receive not just product suggestions but complete menu ideas, cooking techniques, and entertaining tips.</p><p>Thattai, the Salesforce AI executive, said Williams-Sonoma is in what he describes as the second stage of agentic AI maturity. The first stage involves simple question-and-answer interactions. The second involves agents that actually execute business processes. The third — which he said is the largest untapped opportunity — involves agents working proactively in the background.</p><p>Critically, Hasan said Williams-Sonoma does not attempt to disguise its AI agents as human. Customers know they&#x27;re interacting with AI.</p><p>&quot;We don&#x27;t try to hide it,&quot; Hasan said. &quot;We know customers may come in with preconceptions. I&#x27;m sure plenty of people are rolling their eyes thinking, &#x27;I have to deal with this AI thing&#x27;—because their experience with other companies has been that it&#x27;s a cost-cutting maneuver that creates friction.&quot;</p><p>The company surveys customers after AI interactions and benchmarks satisfaction against human-assisted interactions. According to Hasan, the AI now matches human benchmarks — a constraint the company refuses to compromise.</p><p>&quot;We have a high bar for service—a white-glove customer experience,&quot; Hasan said. &quot;AI has to at least maintain that bar. If anything, our goal is to raise it.&quot;</p><p><a href=""https://www.williams-sonoma.com/"">Williams-Sonoma</a> moved from pilot to full production in 28 days, according to Salesforce — a timeline that Thattai said demonstrates how quickly companies can deploy when they build on existing platform infrastructure rather than starting from scratch.</p><h2><b>The three stages of enterprise AI maturity that determine whether companies see ROI</b></h2><p>Beyond the headline customer statistics, Thattai outlined a three-stage maturity framework that he said describes how most enterprises approach agentic AI:</p><p>Stage one involves building simple agents that answer questions — essentially sophisticated chatbots that can access company data to provide accurate, contextual responses. The primary challenge at this stage is ensuring the agent has comprehensive access to relevant information.</p><p>Stage two involves agents that execute workflows — not just answering &quot;what time does my flight leave?&quot; but actually rebooking a flight when a customer asks. Thattai cited Adecco, the recruiting company, as an example of stage-two deployment. The company uses Agentforce to qualify job candidates and match them with roles — a process that involves roughly 30 discrete steps, conditional decisions, and interactions with multiple systems.</p><p>&quot;A large language model by itself can&#x27;t execute a process that complex, because some steps are deterministic and need to run with certainty,&quot; Thattai explained. &quot;Our hybrid reasoning engine uses LLMs for decision-making and reasoning, while ensuring the deterministic steps execute with precision.&quot;</p><p>Stage three — and the one Thattai described as the largest future opportunity — involves agents working proactively in the background without customer initiation. He described a scenario in which a company might have thousands of sales leads sitting in a database, far more than human sales representatives could ever contact individually.</p><p>&quot;Most companies don&#x27;t have the bandwidth to reach out and qualify every one of those customers,&quot; Thattai said. &quot;But if you use an agent to refine profiles and personalize outreach, you&#x27;re creating incremental opportunities that humans simply don&#x27;t have the capacity for.&quot;</p><h2><b>Salesforce edges out Microsoft in analyst rankings of enterprise AI platforms</b></h2><p>The Futurum Group&#x27;s <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">recent analysis</a> of agentic AI platforms placed Salesforce at the top of its rankings, slightly ahead of Microsoft. The report evaluated ten major platforms — including offerings from <a href=""https://aws.amazon.com/"">AWS</a>, <a href=""https://www.google.com/"">Google</a>, <a href=""https://www.ibm.com/us-en"">IBM</a>, <a href=""https://www.oracle.com/"">Oracle</a>, <a href=""https://www.sap.com/index.html"">SAP</a>, <a href=""https://www.servicenow.com/"">ServiceNow</a>, and <a href=""https://www.uipath.com/"">UiPath</a> — across five dimensions: business value, product innovation, strategic vision, go-to-market execution, and ecosystem alignment.</p><p>Salesforce scored above 90 (out of 100) across all five categories, placing it in what the firm calls the &quot;Elite&quot; zone. Microsoft trailed closely behind, with both companies significantly outpacing competitors.</p><p>Thattai acknowledged the competitive pressure but argued that Salesforce&#x27;s existing position in customer relationship management provides structural advantages that pure-play AI companies cannot easily replicate.</p><p>&quot;The richest and most critical data a company has — data about their customers — lives within Salesforce,&quot; Thattai said. &quot;Most of our large customers use us for multiple functions: sales, service, and marketing. That complete view of the customer is central to running any business.&quot;</p><p>The platform advantage extends beyond data. Salesforce&#x27;s existing workflow infrastructure means that AI agents can immediately access business processes that have already been defined and refined — a head start that requires years for competitors to match.</p><p>&quot;Salesforce is not just a place where critical data is put, which it is, but it&#x27;s also where work is performed,&quot; Thattai said. &quot;The process by which a business runs happens in this application — how a sales process is managed, how a marketing process is managed, how a customer service process is managed.&quot;</p><h2><b>Why analysts say 2026 will be the real year of AI agents in the enterprise</b></h2><p>Despite the momentum, both Salesforce executives and independent analysts cautioned that <a href=""https://www.deloitte.com/us/en/what-we-do/capabilities/applied-artificial-intelligence/blogs/pulse-check-series-latest-ai-developments/ai-adoption-challenges-ai-trends.html"">enterprise AI remains in early innings</a>.</p><p>Hinchcliffe pushed back against the notion that 2025 was &quot;the year of agents,&quot; a phrase that circulated widely at the beginning of the year.</p><p>&quot;This was not the year of agents,&quot; Hinchcliffe said. &quot;This was the year of finding out how ready they were, learning the platforms, and discovering where they weren&#x27;t mature yet. The biggest complaint we heard was that there&#x27;s no easy way to manage them. Once companies got all these agents running, they realized: I have to do lifecycle management. I have agents running on old versions, but their processes aren&#x27;t finished. How do I migrate them?&quot;</p><p>He predicted 2026 has &quot;a much more likely chance of being the year of agents,&quot; though added that the &quot;biggest year of agents&quot; is &quot;probably going to be the year after that.&quot;</p><p>The <a href=""https://futurumgroup.com/insights/agentic-ai-platforms-for-enterprise-futurum-signal/"">Futurum Group&#x27;s analysis forecasts</a> the AI platform market growing from $127 billion in 2024 to $440 billion by 2029 — a compound annual growth rate that dwarfs most enterprise software categories.</p><p>For companies still on the sidelines, Salvaggio offered pointed advice based on Engine&#x27;s early-adopter experience.</p><p>&quot;Don&#x27;t take the fast-follower strategy with this technology,&quot; he said. &quot;It feels like it&#x27;s changing every week. There&#x27;s a differentiation period coming — if it hasn&#x27;t started already — and companies that waited are going to fall behind those that moved early.&quot;</p><p>He warned that institutional knowledge about AI deployment is becoming a competitive asset in itself — expertise that cannot be quickly acquired through outside consultants.</p><p>&quot;Companies need to start building AI expertise into their employee base,&quot; Salvaggio said. &quot;You can&#x27;t outsource all of this — you need that institutional knowledge within your organization.&quot;</p><p>Thattai struck a similarly forward-looking note, drawing parallels to previous platform shifts.</p><p>&quot;Think about the wave of mobile technology—apps that created entirely new ways of interacting with companies,&quot; he said. &quot;You&#x27;re going to see that happen with agentic technology. The difference is it will span every channel — voice, chat, mobile, web, text — all tied together by a personalized conversational experience.&quot;</p><p>The question for enterprises is no longer whether AI agents will transform customer and employee experiences. The data from Salesforce&#x27;s customer base suggests that transformation is already underway, generating measurable returns for early adopters willing to invest in platform infrastructure rather than waiting for a theoretical bubble to burst.</p><p>&quot;I feel incredibly confident that point solutions in each of those areas are not the path to getting to an agentic enterprise,&quot; Thattai said. &quot;The platform approach that we&#x27;ve taken to unlock all of this data in this context is really the way that customers are going to get value.&quot;</p>",https://venturebeat.com/technology/while-everyone-talks-about-an-ai-bubble-salesforce-quietly-added-6-000
Red teaming LLMs exposes a harsh truth about the AI security arms race,"Mon, 22 Dec 2025 13:00:00 GMT",VentureBeat,"<p>Unrelenting, persistent attacks on frontier models make them fail, with the patterns of failure varying by model and developer. Red teaming shows that it’s not the sophisticated, complex attacks that can bring a model down; it’s the attacker automating continuous, random attempts that will inevitably force a model to fail.</p><p>That’s the harsh truth that AI apps and platform builders need to plan for as they build each new release of their products. Betting an entire build-out on a frontier model prone to red team failures due to persistency alone is like building a house on sand. Even with red teaming, frontier LLMs, including those with open weights, are lagging behind adversarial and weaponized AI.</p><h2>The arms race has already started</h2><p><a href=""https://cybersecurityventures.com/cybercrime-damage-costs-10-trillion-by-2025/"">Cybercrime costs reached $9.5 trillion in 2024</a> and forecasts exceed $10.5 trillion for 2025. LLM vulnerabilities contribute to that trajectory. A financial services firm deploying a customer-facing LLM without adversarial testing saw it leak internal FAQ content within weeks. Remediation cost $3 million and triggered regulatory scrutiny. One enterprise software company had its entire salary database leaked after executives used an LLM for financial modeling, VentureBeat has learned.</p><p>The UK AISI/Gray Swan challenge ran 1.8 million attacks across 22 models. Every model broke. No current frontier system resists determined, well-resourced attacks.</p><p>Builders face a choice. Integrate security testing now, or explain breaches later. The tools exist — PyRIT, DeepTeam, Garak, OWASP frameworks. What remains is execution.</p><p>Organizations that treat LLM security as a feature rather than a foundation will learn the difference the hard way. The arms race rewards those who refuse to wait.</p><h2>Red teaming reflects how nascent frontier models are </h2><p>The gap between offensive capability and defensive readiness has never been wider. &quot;If you&#x27;ve got adversaries breaking out in two minutes, and it takes you a day to ingest data and another day to run a search, how can you possibly hope to keep up?&quot; <a href=""https://venturebeat.com/security/outsmarting-ai-powered-cyber-attacks-endpoint-defense-2025"">Elia Zaitsev, CTO of CrowdStrike</a>, told VentureBeat back in January. Zaitsev also implied that adversarial AI is progressing so quickly that the traditional tools AI builders trust to power their applications can be weaponized in stealth, jeopardizing product initiatives in the process.</p><p>Red teaming results to this point are a paradox, especially for AI builders who need a stable base platform to build from. Red teaming proves that every frontier model fails under sustained pressure. </p><p>One of my favorite things to do immediately after a new model comes out is to read the system card. It’s fascinating to see how well these documents reflect the red teaming, security, and reliability mentality of every model provider shipping today.</p><p>Earlier this month, I looked at how <a href=""https://venturebeat.com/security/anthropic-vs-openai-red-teaming-methods-reveal-different-security-priorities"">Anthropic’s versus OpenAI’s red teaming practices</a> reveal how different these two companies are when it comes to enterprise AI itself. That’s important for builders to know, as getting locked in on a platform that isn’t compatible with the building team’s priorities can be a massive waste of time.</p><h2>Attack surfaces are moving targets, further challenging red teams</h2><p>Builders need to understand how fluid the attack surfaces are that red teams attempt to cover, despite having incomplete knowledge of the many threats their models will face.</p><p>A good place to start is with one of the best-known frameworks. <a href=""https://genai.owasp.org/llm-top-10/"">OWASP&#x27;s 2025 Top 10 for LLM Applications</a> reads like a cautionary tale for any business building AI apps and attempting to expand on existing LLMs. Prompt injection sits at No. 1 for the second consecutive year. Sensitive information disclosure jumped from sixth to second place. Supply chain vulnerabilities climbed from fifth to third. These rankings reflect production incidents, not theoretical risks.</p><p>Five new vulnerability categories appeared in the 2025 list: excessive agency, system prompt leakage, vector and embedding weaknesses, misinformation, and unbounded consumption. Each represents a failure mode unique to generative AI systems. No one building AI apps can ignore these categories at the risk of shipping vulnerabilities that security teams never detected, or worse, lost track of given how mercurial threat surfaces can change.</p><p>&quot;AI is fundamentally changing everything, and cybersecurity is at the heart of it. We&#x27;re no longer dealing with human-scale threats; these attacks are occurring at machine scale,&quot; <a href=""https://venturebeat.com/security/rsac-2025-why-the-ai-agent-era-means-more-demand-for-cisos"">Jeetu Patel, Cisco&#x27;s President and Chief Product Officer, </a>emphasized to VentureBeat at RSAC 2025. Patel noted that AI-driven models are non-deterministic: &quot;They won&#x27;t give you the same answer every single time, introducing unprecedented risks.&quot;</p><p>&quot;We recognized that adversaries are increasingly leveraging AI to accelerate attacks. With Charlotte AI, we&#x27;re giving defenders an equal footing, amplifying their efficiency and ensuring they can keep pace with attackers in real-time,&quot; <a href=""https://venturebeat.com/security/crowdstrikes-ai-slashes-soc-workloads-over-40-hours-a-week"">Zaitsev told VentureBeat</a>.</p><h2>How and why model providers validate security differently</h2><p>Each frontier model provider wants to prove the security, robustness, and reliability of their system by devising a unique and differentiated red teaming process that is often explained in their system cards.</p><p>From their system cards, it doesn’t take long to see how different each model provider’s approach to red teaming reflects how different each is when it comes to security validation, versioning compatibility or the lack of it, persistence testing, and a willingness to torture-test their models with unrelenting attacks until they break.</p><p>In many ways, red teaming of frontier models is a lot like quality assurance on a commercial jet assembly line. Anthropic’s mentality is comparable to the well-known tests Airbus, Boeing, Gulfstream, and others do. Often called the Wing Bend Test or Ultimate Load Test, the goal of these tests is to push a wing’s strength to the breaking point to ensure the most significant safety margins possible. </p><p>Be sure to read <a href=""https://venturebeat.com/security/anthropic-vs-openai-red-teaming-methods-reveal-different-security-priorities"">Anthropic&#x27;s 153-page system card for Claude Opus 4.5 versus OpenAI&#x27;s 55-page GPT-5 system card</a> to see firsthand how different their measurement philosophies are. Anthropic relies on multi-attempt attack success rates from 200-attempt reinforcement learning campaigns. OpenAI reports single-attempt jailbreak resistance.</p><p><a href=""https://www.grayswan.ai/product/shade"">Gray Swan&#x27;s Shade platform</a> ran a series of adaptive adversarial campaigns against Claude models. The attack success rate (ASR) metrics quantify just how quickly models can degrade and break under pressure. Opus 4.5 in coding environments hit 4.7% ASR at one attempt, 33.6% at 10 attempts, and 63.0% at 100 attempts. In computer use with extended thinking, it held at 0% ASR even after 200 attempts, becoming the first model to saturate the benchmark. Sonnet 4.5 at the same thresholds showed 70% ASR in coding and 85.7% in computer use. The gap between model tiers within the same family is now larger than some cross-vendor comparisons.</p><p>On Gray Swan&#x27;s prompt injection benchmark, Claude Opus 4.5 shows the strongest resistance with an attack success rate of 4.7%, compared with 12.5% for Gemini 3 Pro and 21.9% for GPT-5.1. </p><p>For OpenAI&#x27;s o1, the system card reported 6% ASR for harmful text and 5% for malicious code based on single-attempt metrics. Third-party testing by SPLX and NeuralTrust found GPT-5&#x27;s raw ASR hit 89% initially, dropping below 1% within two weeks of patching. </p><p>The <a href=""https://arxiv.org/abs/2404.02151"">report confirms the pattern</a>: With sufficient attempts, Claude 3.5 Sonnet showed 78% ASR, and GPT-4o reached 89%.</p><h2>M<!-- -->odels attempt to game tests during red teaming — adding to the paradox </h2><p>Models scheme to turn off the oversight mechanisms when they predict they will be shut down. In Apollo Research&#x27;s evaluation of o1, the model attempted to turn off oversight mechanisms 5% of the time when it thought it would be shut down. In 2% of cases, o1 attempted self-exfiltration. When confronted about misaligned actions, o1 denied wrongdoing or fabricated explanations 99% of the time.</p><p>In 37% of scenarios, the model pursued developer-aligned goals during evaluation, then switched to its own goals when it believed oversight was minimal. Anthropic reports Opus 4.5&#x27;s evaluation awareness dropped from 26.5% (Opus 4.1) to less than 10% internally.</p><p><b>Evaluating Anthropic versus OpenAI red teaming results</b></p><table><tbody><tr><td><p><b>Dimension</b></p></td><td><p><b>Claude Opus 4.5</b></p></td><td><p><b>GPT-5</b></p></td><td><p><b>o1</b></p></td></tr><tr><td><p>System card length</p></td><td><p>153 pages</p></td><td><p>55 pages</p></td><td><p>~40 pages</p></td></tr><tr><td><p>Attack methodology</p></td><td><p>200-attempt RL campaigns</p></td><td><p>Single-attempt + patching</p></td><td><p>Single-attempt</p></td></tr><tr><td><p>ASR @ 1 attempt</p></td><td><p>4.7% (coding)</p></td><td><p>89% raw (pre-patch)</p></td><td><p>5-6%</p></td></tr><tr><td><p>ASR @ 200 attempts</p></td><td><p>0% (saturates benchmark)</p></td><td><p>Not published</p></td><td><p>Not published</p></td></tr><tr><td><p>Prompt injection defense</p></td><td><p>96-99.4% prevention</p></td><td><p>~20% ASR (Fujitsu)</p></td><td><p>Higher than GPT-4o</p></td></tr><tr><td><p>Interpretability</p></td><td><p>~10M neural features</p></td><td><p>CoT monitoring</p></td><td><p>CoT monitoring</p></td></tr><tr><td><p>Evaluation awareness</p></td><td><p>&lt;10%</p></td><td><p>Identifies exact eval</p></td><td><p>37% alignment faking</p></td></tr><tr><td><p>Scheming behavior</p></td><td><p>Orders of magnitude lower</p></td><td><p>Not published</p></td><td><p>5% oversight off; 2% self-exfil</p></td></tr></tbody></table><p><i>Sources: Opus 4.5 system card, GPT-5 system card, o1 system card, Gray Swan, METR, Apollo Research</i></p><p>When models attempt to game a red teaming exercise if they anticipate they’re about to be shut down, AI builders need to know the sequence that leads to that logic being created. No one wants a model resisting being shut down in an emergency or commanding a given production process or workflow.</p><h2>Defensive tools struggle against adaptive attackers</h2><p>&quot;Threat actors using AI as an attack vector has been accelerated, and they are so far in front of us as defenders, and we need to get on a bandwagon as defenders to start utilizing AI,&quot; Mike Riemer, Field CISO at Ivanti, told VentureBeat. </p><p>Riemer pointed to patch reverse-engineering as a concrete example of the speed gap: &quot;They&#x27;re able to reverse engineer a patch within 72 hours. So if I release a patch and a customer doesn&#x27;t patch within 72 hours of that release, they&#x27;re open to exploit because that&#x27;s how fast they can now do it,&quot; he noted in a <a href=""https://venturebeat.com/security/weaponized-ai-can-dismantle-patches-in-72-hours-but-ivantis-kernel-defense"">recent VentureBeat interview</a>.</p><p>An <a href=""https://simonwillison.net/2025/Nov/2/new-prompt-injection-papers/"">October 2025 paper from researchers</a> — including representatives from OpenAI, Anthropic, and Google DeepMind — examined 12 published defenses against prompt injection and jailbreaking. Using adaptive attacks that iteratively refined their approach, the researchers bypassed defenses with attack success rates above 90% for most. The majority of defenses had initially been reported to have near-zero attack success rates.</p><p>The gap between reported defense performance and real-world resilience stems from evaluation methodology. Defense authors test against fixed attack sets. Adaptive attackers are very aggressive in using iteration, which is a common theme in all attempts to compromise any model.</p><p>Builders shouldn’t rely on frontier model builders&#x27; claims without also conducting their own testing.</p><p>Open-source frameworks have emerged to address the testing gap. <a href=""https://www.helpnetsecurity.com/2025/11/26/deepteam-open-source-llm-red-teaming-framework/"">DeepTeam, released in November 2025</a>, applies jailbreaking and prompt injection techniques to probe LLM systems before deployment. Garak from Nvidia focuses on vulnerability scanning. MLCommons published safety benchmarks. The tooling ecosystem is maturing, but builder adoption lags behind attacker sophistication.</p><h2>What AI builders need to do now</h2><p>&quot;An AI agent is like giving an intern full access to your network. You gotta put some guardrails around the intern.&quot; <a href=""https://venturebeat.com/security/soc-teams-face-51-second-breach-reality-manual-response-times-are-officially"">George Kurtz, CEO and founder of CrowdStrike, observed at FalCon 2025</a>. That quote typifies the current state of frontier AI models as well. </p><p><a href=""https://ai.meta.com/blog/practical-ai-agent-security/"">Meta&#x27;s Agents Rule of Two</a>, published October 2025, reinforces this principle: Guardrails must live outside the LLM. File-type firewalls, human approvals, and kill switches for tool calls cannot depend on model behavior alone. Builders who embed security logic inside prompts have already lost.</p><p>&quot;Business and technology leaders can&#x27;t afford to sacrifice safety for speed when embracing AI. The security challenges AI introduces are new and complex, with vulnerabilities spanning models, applications, and supply chains. We have to think differently,&quot; <a href=""https://venturebeat.com/security/how-ciscos-ai-defense-stacks-up-against-the-cyber-threats-you-never-see"">Patel told VentureBeat</a> previously.</p><ul><li><p><b>Input validation remains the first line of defense.</b> Enforce strict schemas that define exactly what inputs the LLM endpoints being designed can accept. Reject unexpected characters, escape sequences, and encoding variations. Apply rate limits per user and per session. Create structured interfaces or prompt templates that limit free-form text injection into sensitive contexts.</p></li><li><p><b>Output validation from any LLM or frontier model is a must-have.</b> LLM-generated content passed to downstream systems without sanitization creates classic injection risks: XSS, SQL injection, SSRF, and remote code execution. Treat the model as an untrusted user. Follow OWASP ASVS guidelines for input validation and sanitization.</p></li><li><p><b>Always separate instructions from data.</b> Use different input fields for system instructions and dynamic user content. Prevent user-provided content from being embedded directly into control prompts. This architectural decision prevents entire classes of injection attacks.</p></li><li><p><b>Think of regular red teaming as the muscle memory you always needed; it’s that essential.</b> The <a href=""https://genai.owasp.org/2024/09/12/research-initiative-ai-red-teaming-evaluation/"">OWASP Gen AI Red Teaming Guide</a> provides structured methodologies for identifying model-level and system-level vulnerabilities. Quarterly adversarial testing should become standard practice for any team shipping LLM-powered features.</p></li><li><p><b>Control agent permissions ruthlessly.</b> For LLM-powered agents that can take actions, minimize extensions and their functionality. Avoid open-ended extensions. Execute extensions in the user&#x27;s context with their permissions. Require user approval for high-impact actions. The principle of least privilege applies to AI agents just as it applies to human users.</p></li><li><p><b>Supply chain scrutiny cannot wait.</b> Vet data and model sources. Maintain a software bill of materials for AI components using tools like OWASP CycloneDX or ML-BOM. Run custom evaluations when selecting third-party models rather than relying solely on public benchmarks.</p></li></ul><p></p>",https://venturebeat.com/security/red-teaming-llms-harsh-truth-ai-security-arms-race
From assistance to autonomy: How agentic AI is redefining enterprises,"Mon, 22 Dec 2025 05:00:00 GMT",VentureBeat,"<p><i>Presented by EdgeVerve</i></p><hr /><p>Artificial intelligence (AI) has long promised to change the way enterprises operate. For years, the focus was on assistants, systems that could surface information, summarize documents, or streamline repetitive tasks. While valuable, these technological assistants were reactive: they waited for human prompts and provided limited support within narrow boundaries.</p><p>Today, a new chapter is unfolding. Agentic AI, whose systems are capable of autonomous decision-making and multi-step orchestration, represents a significant evolution. These systems don’t just assist, they act. They evaluate context, weigh outcomes and autonomously initiate actions, orchestrating complex workflows across functions. They adapt dynamically and collaborate with other agents in ways that are beginning to reshape enterprise operations at large. </p><p>For leaders, this shift carries both opportunity and responsibility. The potential is immense, but so are the governance, trust and design challenges that come with giving AI systems greater autonomy. Enterprises must be able to monitor and override any actions taken by the agentic AI systems.</p><h3>Shift from assistance to autonomy</h3><p>Traditional AI assistants primarily respond to queries and perform isolated tasks. They are helpful but constrained. Agentic AI pushes further: multiple agents can collaborate, exchange context and manage workflows end-to-end.</p><p>Imagine a procurement workflow. An assistant can pull vendor data or draft a purchase order. An agentic system, however, can review demand forecasts, evaluate vendor risk, check compliance policies, negotiate terms and finalize transactions. It does this all while coordinating across global business departments, including finance, operations and compliance.</p><p>This shift from narrow support to autonomous orchestration is the defining leap of the next era of enterprise AI. It is not about replacing humans but about embedding intelligence into the very fabric of organizational workflows.</p><h3>Rethink enterprise workflows </h3><p>The goal of every enterprise department is focused on efficiency, scale and standardization. But agentic AI challenges enterprises to think differently. Instead of designing workflows step by step and inserting automation, organizations now need to completely reimagine and architect intelligent ecosystems for orchestrating processes, adapting to evolving business needs, and enabling seamless collaboration between humans and agents.</p><p>That requires new thinking. Which decisions should remain human-led, and which can be delegated? How do you ensure agents access the correct data without overstepping boundaries? What happens when agents from finance, HR and supply chain must coordinate autonomously?</p><p>The design of workflows is no longer about linear handoffs; it is about orchestrated ecosystems. Enterprises that get this right can achieve speed and agility that traditional automation cannot match.</p><h3>Accelerate agentic AI-led transformation with a unified platform</h3><p>In this environment, unified platforms become critical. Without them, enterprises risk a proliferation of disconnected agents working at cross-purposes. A unified approach provides the guardrails with shared knowledge graphs, consistent policy frameworks and a single orchestration layer that ensures interoperability across business functions.</p><p>This platform-based approach not only reduces complexity but also enables scale. Enterprises don’t want dozens of fragmented AI projects that stall in the pilot stages. They want enterprise-grade systems where agents can collaborate securely and consistently across the enterprise.</p><p>Unified platforms simplify outcome monitoring and strengthen governance  —both critical as systems become increasingly autonomous.</p><h3>Build trust and accountability</h3><p>As AI systems act with greater independence, the stakes rise. An agent who makes flawed decisions in customer service may frustrate a client. An agent that mishandles a compliance process could expose the enterprise to regulatory risk.</p><p>That’s why trust and accountability must be designed into agentic AI from the start. Governance is not an afterthought; it is a foundation. Leaders need clear policies defining the scope of agentic autonomy, transparent logging of decisions, evaluating and monitoring agents and escalation mechanisms when human oversight is required.</p><p>Equally important is cultural trust. Employees must believe these systems are partners, not threats. This calls for change management, training, and communication that positions agentic AI as augmenting human capability rather than replacing it.</p><h3>Measure business value early</h3><p>One of the most common pitfalls in enterprise AI adoption is the gap between promising pilots and at-scale results. Studies show that a significant percentage of AI projects never make it past experimentation. Agentic AI cannot afford to fall into this trap.</p><p>Enterprises must measure business value early and continuously. This includes efficiency gains, cost reductions, error avoidance and even intangible benefits like faster decision-making or improved compliance. Success will be defined by automation coverage across processes, reductions in manual intervention and the ability to deliver new services at speed and scale.</p><p>When designed responsibly, agentic AI can deliver exponential improvements. A procurement cycle reduced from weeks to hours, or a compliance review automated at scale, can fundamentally alter enterprise performance.</p><h3>Preparing for the future</h3><p>The rise of agentic AI does not mean handing over control to machines or codes. Instead, it marks the next phase of enterprise transformation, where humans and agents operate side by side in orchestrated systems.</p><p>Leaders should start by piloting agentic systems in well-defined domains with clear governance models. From there, scaling across the enterprise requires investment in unified platforms, robust policy frameworks, and a culture that embraces intelligent automation as a partner in value creation.</p><p>The enterprises that succeed will be those that approach agentic AI not as another tool, but as a strategic shift. Just as ERP and cloud once redefined operations, agentic AI is poised to do the same, reshaping workflows, governance, and the very way decisions are made.</p><p>Agentic AI is moving the enterprise conversation from assistance to autonomy. That change comes with objective complexity, but also with extraordinary promise. The foundation for success lies in unified platforms that enable enterprises to orchestrate with intelligence, govern with trust, and scale with confidence. </p><p>The journey is just beginning. And for enterprise leaders, now is the time to lead with vision, responsibility, and ambition.</p><p><i>N Shashidhar is VP and Global Platform Head of EdgeVerve AI Next</i>.</p><hr /><p><i>Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact </i><a href=""mailto:sales@venturebeat.com""><i><u>sales@venturebeat.com</u></i></a><i>.</i></p>",https://venturebeat.com/technology/from-assistance-to-autonomy-how-agentic-ai-is-redefining-enterprises
Agent autonomy without guardrails is an SRE nightmare,"Sun, 21 Dec 2025 19:00:00 GMT",VentureBeat,"<p><i>João Freitas is GM and VP of engineering for AI and automation at </i><a href=""https://www.pagerduty.com/""><i>PagerDuty</i></a></p><p>As AI use continues to evolve in large organizations, leaders are increasingly seeking the next development that will yield major ROI. The latest wave of this ongoing trend is the adoption of AI agents. However, as with any new technology, organizations must ensure they adopt AI agents in a responsible way that allows them to facilitate both speed and security. </p><p><a href=""https://venturebeat.com/ai/hiring-specialists-made-sense-before-ai-now-generalists-win"">More than half</a> of organizations have already deployed AI agents to some extent, with more expecting to follow suit in the next two years. But many early adopters are now reevaluating their approach. Four-in-10 tech leaders regret not establishing a <a href=""https://venturebeat.com/ai/why-observable-ai-is-the-missing-sre-layer-enterprises-need-for-reliable"">stronger governance foundation</a> from the start, which suggests they adopted AI rapidly, but with margin to improve on policies, rules and best practices designed to ensure the responsible, ethical and legal development and use of AI.</p><p><a href=""https://venturebeat.com/ai/hiring-specialists-made-sense-before-ai-now-generalists-win"">As AI adoption accelerates</a>, organizations must find the right balance between their exposure risk and the implementation of guardrails to ensure AI use is secure.</p><h2><b>Where do AI agents create potential risks?</b></h2><p>There are three principal areas of consideration for safer AI adoption.</p><p>The first is <a href=""https://venturebeat.com/security/shadow-ai-doubles-every-18-months-creating-blind-spots-socs-never-see"">shadow AI</a>, when employees use unauthorized AI tools without express permission, bypassing approved tools and processes. IT should create necessary processes for experimentation and innovation to introduce more efficient ways of working with AI. While shadow AI has existed as long as AI tools themselves, AI agent autonomy makes it easier for unsanctioned tools to operate outside the purview of IT, which can introduce fresh security risks.</p><p>Secondly, organizations must close gaps in AI ownership and accountability to prepare for incidents or processes gone wrong. The strength of AI agents lies in their autonomy. However, if agents act in unexpected ways, teams must be able to determine who is responsible for addressing any issues.</p><p>The third risk arises when there is a lack of explainability for actions AI agents have taken. <a href=""https://venturebeat.com/ai/ontology-is-the-real-guardrail-how-to-stop-ai-agents-from-misunderstanding"">AI agents are goal-oriented</a>, but how they accomplish their goals can be unclear. AI agents must have explainable logic underlying their actions so that engineers can trace and, if needed, roll back actions that may cause issues with existing systems.</p><p>While none of these risks should delay adoption, they will help organizations better ensure their security.</p><h2><b>The three guidelines for responsible AI agent adoption</b></h2><p>Once organizations have identified the risks AI agents can pose, they must implement guidelines and guardrails to ensure safe usage. By following these three steps, organizations can minimize these risks.</p><p><b>1: Make human oversight the default </b></p><p>AI agency continues to evolve at a fast pace. However, we still need human oversight when AI agents are given the  capacity to act, make decisions and pursue a goal that may impact key systems. A human should be in the loop by default, especially for business-critical use cases and systems. The teams that use AI must understand the actions it may take and where they may need to intervene. Start conservatively and, over time, increase the level of agency given to AI agents.</p><p>In conjunction, operations teams, engineers and security professionals must understand the role they play in supervising AI agents’ workflows. Each agent should be assigned a specific human owner for clearly defined oversight and accountability. Organizations must also allow any human to flag or override an AI agent’s behavior when an action has a negative outcome.</p><p>When considering tasks for AI agents, organizations should understand that, while traditional automation is good at handling repetitive, rule-based processes with structured data inputs, AI agents can handle much more complex tasks and adapt to new information in a more autonomous way. This makes them an appealing solution for all sorts of tasks. But as AI agents are deployed, organizations should control what actions the agents can take, particularly in the early stages of a project. Thus, teams working with AI agents should have approval paths in place for high-impact actions to ensure agent scope does not extend beyond expected use cases, minimizing risk to the wider system.</p><p><b>2: Bake in security </b></p><p>The introduction of new tools should not expose a system to fresh security risks. </p><p>Organizations should consider agentic platforms that comply with high security standards and are validated by enterprise-grade certifications such as SOC2, FedRAMP or equivalent. Further, AI agents should not be allowed free rein across an organization’s systems. At a minimum, the permissions and security scope of an AI agent must be aligned with the scope of the owner, and any tools added to the agent should not allow for extended permissions. Limiting AI agent access to a system based on their role will also ensure deployment runs smoothly. Keeping complete logs of every action taken by an AI agent can also help engineers understand what happened in the event of an incident and trace back the problem.</p><p><b>3: Make outputs explainable </b></p><p>AI use in an organization must never be a black box. The reasoning behind any action must be illustrated so that any engineer who tries to access it can understand the context the agent used for decision-making and access the traces that led to those actions.</p><p>I<!-- -->nputs and outputs for every action should be logged and accessible. This will help organizations establish a firm overview of the logic underlying an AI agent’s actions, providing significant value in the event anything goes wrong.</p><h2><b>Security underscores AI agents’ success</b></h2><p>AI agents offer a huge opportunity for organizations to accelerate and improve their existing processes. However, if they do not prioritize security and strong governance, they could expose themselves to new risks. </p><p>As AI agents become more common, organizations must ensure they have systems in place to measure how they perform and the ability to take action when they create problems.</p><p><i>Read more from our </i><a href=""https://venturebeat.com/datadecisionmakers""><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=""https://venturebeat.com/guest-posts""><i>guidelines here</i></a><i>. </i></p>",https://venturebeat.com/infrastructure/agent-autonomy-without-guardrails-is-an-sre-nightmare
Hiring specialists made sense before AI — now generalists win,"Sat, 20 Dec 2025 19:00:00 GMT",VentureBeat,"<p><i>Tony Stoyanov is CTO and co-founder of </i><a href=""https://eliseai.com/""><i>EliseAI</i></a></p><p>In the 2010s, tech companies chased staff-level specialists: Backend engineers, data scientists, system architects. That model worked when technology evolved slowly. Specialists knew their craft, could deliver quickly and built careers on predictable foundations like cloud infrastructure or the latest JS framework</p><p>Then AI went mainstream.</p><p>The pace of change has exploded. New technologies appear and mature in less than a year. You can’t hire someone who has been <a href=""https://venturebeat.com/ai/build-vs-buy-is-dead-ai-just-killed-it"">building AI agents</a> for five years, as the technology hasn’t existed for that long. The people thriving today aren’t those with the longest résumés; they’re the ones who learn fast, adapt fast and act without waiting for direction. Nowhere is this transformation more evident than in software engineering, which has likely experienced the most dramatic shift of all, evolving faster than almost any other field of work.</p><h2><b>How AI Is rewriting the rules</b></h2><p>AI has lowered the barrier to doing complex technical work, technical skills and it&#x27;s also raised expectations for what counts as real expertise. McKinsey estimates that by 2030, <a href=""https://www.mckinsey.com/mgi/our-research/generative-ai-and-the-future-of-work-in-america?utm_source=chatgpt.com"">up to 30% of U.S. work hours</a> could be automated and 12 million workers may need to shift roles entirely. Technical depth still matters, but AI favors people who can figure things out as they go.</p><p>At my company, I see this every day. Engineers who never touched <a href=""https://venturebeat.com/ai/why-most-enterprise-ai-coding-pilots-underperform-hint-its-not-the-model"">front-end code</a> are now building UIs, while front-end developers are moving into back-end work. The technology keeps getting easier to use but the problems are harder because they span more disciplines.</p><p>In that kind of environment, being great at one thing isn’t enough. What matters is the ability to bridge engineering, product and operations to make good decisions quickly, even with imperfect information.</p><p>Despite all the excitement, <a href=""https://venturebeat.com/ai/why-most-enterprise-ai-coding-pilots-underperform-hint-its-not-the-model"">only 1% of companies</a> consider themselves truly mature in how they use AI. Many still rely on structures built for a slower era — layers of approval, rigid roles and an overreliance on specialists who can’t move outside their lane.</p><h2><b>The traits of a strong generalist </b></h2><p>A strong generalist has breadth without losing depth. They go deep in one or two domains but stay fluent across many. As David Epstein puts it in <i>Range</i>, “You have people walking around with all the knowledge of humanity on their phone, but they have no idea how to integrate it. We don’t train people in thinking or reasoning.” True expertise comes from connecting the dots, not just collecting information.</p><p>The best generalists share these traits:</p><ul><li><p><b>Ownership:</b> End-to-end accountability for outcomes, not just tasks.</p></li><li><p><b>First-principles thinking:</b> Question assumptions, focus on the goal, and rebuild when needed.</p></li><li><p><b>Adaptability:</b> Learn new domains quickly and move between them smoothly.</p></li><li><p><b>Agency:</b> Act without waiting for approval and adjust as new information comes in.</p></li><li><p><b>Soft skills:</b> Communicate clearly, align teams and keep customers’ needs in focus.</p></li><li><p><b>Range:</b> Solve different kinds of problems and draw lessons across contexts.</p></li></ul><p>I try to make accountability a priority for my teams. Everyone knows what they own, what success looks like and how it connects to the mission. Perfection isn’t the goal, forward movement is.</p><h2><b>Embracing the shift</b></h2><p>Focusing on adaptable builders changed everything. These are the people with the range and <a href=""https://venturebeat.com/ai/why-ai-coding-agents-arent-production-ready-brittle-context-windows-broken"">curiosity to use AI tools</a> to learn quickly and execute confidently.</p><p>If you’re a builder who thrives in ambiguity, this is your time. The AI era rewards curiosity and initiative more than credentials. If you’re hiring, look ahead. The people who’ll move your company forward might not be the ones with the perfect résumé for the job. They’re the ones who can grow into what the company will need as it evolves.</p><p>The future belongs to generalists and to the companies that trust them.</p><p><i>Read more from our </i><a href=""https://venturebeat.com/datadecisionmakers""><i>guest writers</i></a><i>. Or, consider submitting a post of your own! See our </i><a href=""https://venturebeat.com/guest-posts""><i>guidelines here</i></a><i>. </i></p>",https://venturebeat.com/technology/hiring-specialists-made-sense-before-ai-now-generalists-win
Google releases FunctionGemma: a tiny edge model that can control mobile devices with natural language,"Fri, 19 Dec 2025 19:57:00 GMT",VentureBeat,"<p>While Gemini 3 is still making waves, Google&#x27;s not taking the foot off the gas in terms of releasing new models.</p><p>Yesterday, the<a href=""https://blog.google/technology/developers/functiongemma/""> company released FunctionGemma</a>, a specialized 270-million parameter AI model designed to solve one of the most persistent bottlenecks in modern application development: reliability at the edge. </p><p>Unlike general-purpose chatbots, FunctionGemma is engineered for a single, critical utility—translating natural language user commands into structured code that apps and devices can actually execute, all without connecting to the cloud.</p><p>The release marks a significant strategic pivot for Google DeepMind and the Google AI Developers team. While the industry continues to chase trillion-parameter scale in the cloud, FunctionGemma is a bet on &quot;Small Language Models&quot; (SLMs) running locally on phones, browsers, and IoT devices. </p><p>For AI engineers and enterprise builders, this model offers a new architectural primitive: a privacy-first &quot;router&quot; that can handle complex logic on-device with negligible latency.</p><div></div><p>FunctionGemma is available immediately for download on <a href=""https://huggingface.co/collections/google/functiongemma"">Hugging Face</a> and <a href=""https://www.kaggle.com/models/google/functiongemma"">Kaggle</a>. You can also see the model in action by downloading the Google AI Edge Gallery app on the Google Play Store.</p><h3><b>The Performance Leap</b></h3><p>At its core, FunctionGemma addresses the &quot;execution gap&quot; in generative AI. Standard large language models (LLMs) are excellent at conversation but often struggle to reliably trigger software actions—especially on resource-constrained devices.</p><p>According to Google’s internal &quot;Mobile Actions&quot; evaluation, a generic small model struggles with reliability, achieving only a 58% baseline accuracy for function calling tasks. However, once fine-tuned for this specific purpose, FunctionGemma’s accuracy jumped to 85%, creating a specialized model that can exhibit the same success rate as models many times its size.</p><p>It allows the model to handle more than just simple on/off switches; it can parse complex arguments, such as identifying specific grid coordinates to drive game mechanics or detailed logic.</p><p>The release includes more than just the model weights. Google is providing a full &quot;recipe&quot; for developers, including:</p><ul><li><p>The Model: A 270M parameter transformer trained on 6 trillion tokens.</p></li><li><p>Training Data: A &quot;Mobile Actions&quot; dataset to help developers train their own agents.</p></li><li><p>Ecosystem Support: Compatibility with Hugging Face Transformers, Keras, Unsloth, and NVIDIA NeMo libraries.</p></li></ul><p>Omar Sanseviero, Developer Experience Lead at Google DeepMind, highlighted the versatility of the <a href=""https://x.com/osanseviero/status/2001704034667769978?s=20"">release on X</a> (formerly Twitter), noting the model is &quot;designed to be specialized for your own tasks&quot; and can run in &quot;your phone, browser or other devices.&quot;</p><p>This local-first approach offers three distinct advantages:</p><ul><li><p>Privacy: Personal data (like calendar entries or contacts) never leaves the device.</p></li><li><p>Latency: Actions happen instantly without waiting for a server round-trip. The small size means the speed at which it processes input is significant, particularly with access to accelerators such as GPUs and NPUs.</p></li><li><p>Cost: Developers don&#x27;t pay per-token API fees for simple interactions.</p></li></ul><h3><b>For AI Builders: A New Pattern for Production Workflows</b></h3><p>For enterprise developers and system architects, FunctionGemma suggests a move away from monolithic AI systems toward compound systems. Instead of routing every minor user request to a massive, expensive cloud model like GPT-4 or Gemini 1.5 Pro, builders can now deploy FunctionGemma as an intelligent &quot;traffic controller&quot; at the edge.</p><p>Here is how AI builders should conceptualize using FunctionGemma in production:</p><p><b>1. The &quot;Traffic Controller&quot; Architecture: </b>In a production environment, FunctionGemma can act as the first line of defense. It sits on the user&#x27;s device, instantly handling common, high-frequency commands (navigation, media control, basic data entry). If a request requires deep reasoning or world knowledge, the model can identify that need and route the request to a larger cloud model. This hybrid approach drastically reduces cloud inference costs and latency. This enables use cases such as routing queries to the appropriate sub-agent.</p><p><b>2. Deterministic Reliability over Creative Chaos:</b> Enterprises rarely need their banking or calendar apps to be &quot;creative.&quot; They need them to be accurate. The jump to 85% accuracy confirms that specialization beats size. Fine-tuning this small model on domain-specific data (e.g., proprietary enterprise APIs) creates a highly reliable tool that behaves predictably—a requirement for production deployment.</p><p><b>3. Privacy-First Compliance: </b>For sectors like healthcare, finance, or secure enterprise ops, sending data to the cloud is often a compliance risk. Because FunctionGemma is efficient enough to run on-device (compatible with NVIDIA Jetson, mobile CPUs, and browser-based Transformers.js), sensitive data like PII or proprietary commands never has to leave the local network.</p><h3><b>Licensing: Open-ish With Guardrails</b></h3><p>FunctionGemma is released under Google&#x27;s custom <a href=""https://ai.google.dev/gemma/terms"">Gemma Terms of Use</a>. For enterprise and commercial developers, this is a critical distinction from standard open-source licenses like MIT or Apache 2.0.</p><p>While Google describes Gemma as an &quot;open model,&quot; it is not strictly &quot;Open Source&quot; by the Open Source Initiative (OSI) definition. </p><p>The license allows for free commercial use, redistribution, and modification, but it includes specific Usage Restrictions. Developers are prohibited from using the model for restricted activities (such as generating hate speech or malware), and Google reserves the right to update these terms.</p><p>For the vast majority of startups and developers, the license is permissive enough to build commercial products. However, teams building dual-use technologies or those requiring strict copyleft freedom should review the specific clauses regarding &quot;Harmful Use&quot; and attribution.</p><p><i>Correction: This article mistakenly listed Omar Sanseviero as Developer Lead at Hugging Face, his prior role. It has since been updated to his correct role at Google DeepMind. We apologize and regret the error.</i></p>",https://venturebeat.com/technology/google-releases-functiongemma-a-tiny-edge-model-that-can-control-mobile
Even Google and Replit struggle to deploy AI agents reliably — here's why,"Fri, 19 Dec 2025 19:00:00 GMT",VentureBeat,"<p>2025 was supposed to be the year of the AI agent, right? </p><p>Not quite, acknowledge Google Cloud and Replit — two big players in the AI agent space and partners in the &quot;<a href=""https://cloud.google.com/blog/products/ai-machine-learning/bringing-vibe-coding-to-the-enterprise-with-replit""><u>vibe coding</u></a>&quot; movement — at a recent VB Impact Series event.</p><p>Even as they build out agentic tools themselves, leaders from the two companies say the capabilities aren’t quite there yet. </p><p>This constrained reality comes down to struggles with legacy workflows, fragmented data, and immature governance models. Also, enterprises fundamentally misunderstand that agents aren’t like other technologies: They require a fundamental rethink and reworking of workflows and processes. </p><p>When enterprises are building agents to automate work, “most of them are toy examples,” Amjad Masad, CEO and founder of Replit, said during the event. “They get excited, but when they start rolling it out, it&#x27;s not really working very well.”</p><h2>Building agents based on Replit’s own mistakes</h2><p>Reliability and integration, rather than intelligence itself, are two primary barriers to AI agent success, Masad noted. Agents frequently fail when run for extended periods, accumulate errors, or lack access to clean, well-structured data. </p><p>The problem with enterprise data is it’s messy — it’s structured, unstructured, and stored all over the place — and crawling it is a challenge. Added to that, there are many unwritten things that people do that are difficult to encode in agents, Masad said. </p><p>“The idea that companies are just going to turn on agents and agents will replace workers or do workflow automations automatically, it&#x27;s just not the case today,” he said. “The tooling is not there.” </p><p>Going beyond agents are computer use tools, which can take over a user’s workspace for basic tasks like web browsing. But these are still in their infancy and can be buggy, unreliable, and even dangerous, despite the accelerated hype. </p><p>“The problem is computer use models are really bad right now,” Masad said. “They&#x27;re expensive, they&#x27;re slow, they&#x27;re making progress, but they&#x27;re only about a year old.” </p><p>Replit is learning from its own blunder earlier this year, when its AI coder wiped <a href=""https://www.businessinsider.com/replit-ceo-apologizes-ai-coding-tool-delete-company-database-2025-7""><u>a company&#x27;s entire code base</u></a> in a test run. Masad conceded: “The tools were not mature enough,” noting that the company has since isolated development from production. </p><p>Techniques such as testing-in-the-loop, verifiable execution, and development isolation are essential, he noted, even as they can be highly resource-intensive. Replit incorporated in-the-loop capabilities into version 3 of its agent, and Masad said that its next-gen agent can work autonomously for 200 minutes; some have run it for 20 hours. </p><p>Still, he acknowledged that users have expressed frustration around lag times. When they put in a “hefty prompt,” they may have to wait 20 minutes or longer. Ideally, they’ve expressed that they want to be involved in more of a creative loop where they can enter numerous prompts, work on multiple tasks at once, and adjust the design as the agent is working. </p><p>“The way to solve that is parallelism, to create multiple agent loops and have them work on these independent features while allowing you to do the creative work at the same time,” he said. </p><h2>Agents require a cultural shift</h2><p>Beyond the technical perspective, there’s a cultural hurdle: Agents operate probabilistically, but traditional enterprises are structured around deterministic processes, noted Mike Clark, director of product development at Google Cloud. This creates a cultural and operational mismatch as LLMs steam in with all-new tools, orchestration frameworks and processes. </p><p>“We don&#x27;t know how to think about agents,” Clark said. “We don&#x27;t know how to solve for what agents can do.”</p><p>The companies doing it right are being driven by bottoms-up processes, he noted: no-code and low-code software and tool creation in the trenches funneling up to larger agents. As of yet, the deployments that are successful are narrow, carefully scoped and heavily supervised. </p><p>“If I look at 2025 and this promise of it being the year of agents, it was the year a lot of folks spent building prototypes,” Clark said. “Now we’re in the middle of this huge scale phase.”</p><h2>How do you secure a pasture-less world?</h2><p>Another struggle is AI agent security, which also requires a rethink of traditional processes, Clark noted.  </p><p>Security perimeters have been drawn around everything — but that doesn’t work when agents need to be able to access many different resources to make the best decisions, said Clark. </p><p>“It&#x27;s really changing our security models, changing our base level,” he said. “What does least privilege mean in a pasture-less defenseless world?”</p><p>Ultimately, there must be a governance rethink on the part of the whole industry, and enterprises must align on a threat model around agents. </p><p>Clark pointed out the disparity: “If you look at some of your governance processes, you&#x27;ll be very surprised that the origin of those processes was somebody on an IBM electric typewriter typing in triplicate and handing that to three people. That is not the world we live in today.” </p>",https://venturebeat.com/orchestration/even-google-and-replit-struggle-to-deploy-ai-agents-reliably-heres-why
